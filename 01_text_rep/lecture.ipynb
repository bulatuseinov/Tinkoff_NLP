{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение в диалоговых системах\n",
    "\n",
    "Рады вас видеть на курсе по разработке диалоговых систем от Тинькофф Финтех школы! \n",
    "\n",
    "В рамках курса мы рассмотрим, как **обработка естественного языка** и **машинное обучение** используются для построения чат-бот систем, от начала и до конца."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bender.gif\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта лекция вводная, в ней мы рассмотрим:\n",
    "* предобработку текста\n",
    "* представление текста\n",
    "* понятие эмбеддинга\n",
    "* текстовую классификацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Предобработка текста\n",
    "\n",
    "Текст на естественном языке, который нужно обрабатывать в задачах машинного обучения, сильно зависит от источника. Пример:\n",
    "\n",
    "Википедия\n",
    "> Литературный язык — обработанная часть общенародного языка, обладающая в большей или меньшей степени письменно закреплёнными нормами; язык всех проявлений культуры, выражающихся в словесной форме.\n",
    "\n",
    "Твиттер\n",
    "> Если у вас в компании есть люди, которые целый день сидят в чатиках и смотрят видосики, то, скорее всего, это ДАТАСАЕНТИСТЫ и у них ОБУЧАЕТСЯ\n",
    "\n",
    "Ответы@Mail.ru\n",
    "> как пишется \"Вообщем лето было отличное\" раздельно или слитно слово ВОобщем?? ?\n",
    "\n",
    "В связи с этим, возникает задача предобработки (или нормализации) текста, то есть приведения к некоторому единому виду.\n",
    "\n",
    "**Quiz: Какие шаги/действия можно производить и что это даст?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text = 'купил таблетки от тупости, но не смог открыть банку,ЧТО ДЕЛАТЬ???'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 1.1 Приведение текста к нижнему регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'купил таблетки от тупости, но не смог открыть банку,что делать???'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Удаление неинформативных символов.\n",
    "\n",
    "Такими символами могут быть символы пунктуации, спец-символы, повторяющиеся символы, цифры. Для удаления подобных символов можно пользоваться стандартной библиотекой для [регулярных выражений](https://docs.python.org/3/library/re.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'купил таблетки от тупости, но не смог открыть банку,что делать?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile(r'(\\W)\\1+')\n",
    "regex.sub(r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'купил таблетки от тупости  но не смог открыть банку что делать'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile(r'[^\\w\\s]')\n",
    "regex.sub(r' ', text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Разбиение текста на смысловые единицы (токенизация)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Купите кружку-термос \"Hello Kitty\" на 0.5л (64см³) за 3 рубля. До 01.01.2050.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой подход к токенизации - это разбиение по текста по пробельным символам. \n",
    "\n",
    "**Quiz: Какая у этого подхода есть проблема?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '\"Hello',\n",
       " 'Kitty\"',\n",
       " 'на',\n",
       " '0.5л',\n",
       " '(64см³)',\n",
       " 'за',\n",
       " '3',\n",
       " 'рубля.',\n",
       " 'До',\n",
       " '01.01.2050.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке для морфологического анализа для русского языка [`pymorphy2`](https://pymorphy2.readthedocs.io/en/latest/) есть простая вспомогательная функция для токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '\"',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " '\"',\n",
       " 'на',\n",
       " '0',\n",
       " '.',\n",
       " '5л',\n",
       " '(',\n",
       " '64см³',\n",
       " ')',\n",
       " 'за',\n",
       " '3',\n",
       " 'рубля',\n",
       " '.',\n",
       " 'До',\n",
       " '01',\n",
       " '.',\n",
       " '01',\n",
       " '.',\n",
       " '2050',\n",
       " '.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "\n",
    "simple_word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более сложной метод токенизации представлен в [`nltk`](https://www.nltk.org/): библиотеке для общего NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '``',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " \"''\",\n",
       " 'на',\n",
       " '0.5л',\n",
       " '(',\n",
       " '64см³',\n",
       " ')',\n",
       " 'за',\n",
       " '3',\n",
       " 'рубля',\n",
       " '.',\n",
       " 'До',\n",
       " '01.01.2050',\n",
       " '.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского языка также есть новая специализированная библиотека [`razdel`](https://github.com/natasha/razdel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '\"',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " '\"',\n",
       " 'на',\n",
       " '0.5',\n",
       " 'л',\n",
       " '(',\n",
       " '64',\n",
       " 'см³',\n",
       " ')',\n",
       " 'за',\n",
       " '3',\n",
       " 'рубля',\n",
       " '.',\n",
       " 'До',\n",
       " '01.01.2050',\n",
       " '.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import razdel\n",
    "\n",
    "\n",
    "def tokenize_with_razdel(text):\n",
    "    return [token.text for token in razdel.tokenize(text)]\n",
    "\n",
    "\n",
    "tokenize_with_razdel(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Приведение слов к нормальной форме (стемминг, лемматизация)\n",
    "\n",
    "**Стемминг - это нормализация слова путём отбрасывания окончания по правилам языка.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая нормализация хорошо подходит для языков с небольшим разнообразием словоформ, например, для английского. В библиотеке [nltk](https://www.nltk.org/) есть несколько реализаций стеммеров:\n",
    " - [Porter stemmer](http://tartarus.org/martin/PorterStemmer/)\n",
    " - [Snowball stemmer](http://snowball.tartarus.org/)\n",
    " - [Lancaster stemmer](http://www.nltk.org/_modules/nltk/stem/lancaster.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "SnowballStemmer(language='english').stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write wrote written'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "SnowballStemmer(language='english').stem('write wrote written')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского языка этот подход не очень подходит, поскольку в русском есть падежные формы, время у глаголов и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бежа'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer(language='russian').stem('бежать')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Лемматизация - приведение слов к начальной морфологической форме (с помощью словаря и грамматики языка).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для лемматизации русских слов есть несколько библиотек в свободном доступе:\n",
    "- [pymorphy2](https://pymorphy2.readthedocs.io/en/latest/)\n",
    "- [mystem3](https://tech.yandex.ru/mystem/)\n",
    "- [maru](https://github.com/chomechome/maru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой подход к лемматизации - словарный. Здесь не учитывается контекст слова, поэтому для омонимов такой подход работает не всегда. Такой подход применяет библиотека `pymorphy2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "pymorphy = MorphAnalyzer()\n",
    "\n",
    "\n",
    "def lemmatize_with_pymorphy(tokens):\n",
    "    return [pymorphy.parse(token)[0].normal_form for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мама', 'мыло', 'рам']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_pymorphy(['мама', 'мыла', 'раму'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека от Яндекса `mystem3` обходит это ограничение и рассматривает контекст слова, используя статистику и правила."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "\n",
    "def lemmatize_with_mystem(text):\n",
    "    return [lemma for lemma in mystem.lemmatize(text) if not lemma.isspace()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мама', 'мыть', 'рама']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_mystem('мама мыла раму')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но на более сложных примерах такой подход тоже может сойтись к самому частотному варианту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на', 'завод', 'становиться', 'увидеть', 'вид', 'становиться']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_mystem('на заводе стали увидел виды стали')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека `maru` использует машинное обучение и нейросети для разрешения омонимии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import maru\n",
    "\n",
    "maru_rnn = maru.get_analyzer('rnn')\n",
    "\n",
    "\n",
    "def lemmatize_with_maru(tokens):\n",
    "    return [morph.lemma for morph in maru_rnn.analyze(tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на', 'завод', 'сталь', 'увидеть', 'вид', 'сталь']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_with_maru(['на', 'заводе', 'стали', 'увидел', 'виды', 'стали'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Представление текста\n",
    "\n",
    "**Quiz: Как можно использовать токенизированные тексты в задачах NLP? Какие варианты представления текста можете назвать?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 One-Hot Encoding\n",
    "<img src=\"one_hot.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Что такое разреженная матрица?**\n",
    "\n",
    "**Quiz: Какие есть плюсы и минусы у one-hot представления?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как получить one-hot? \n",
    "\n",
    "Сначала нам нужно каждому слову поставить в соответствие номер, а затем перевести их в бинарные вектора. \n",
    "\n",
    "Используем библиотеку [`scikit-learn`](https://scikit-learn.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T11:44:20.487638Z",
     "start_time": "2019-01-30T11:44:20.482337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "words = ['NLP', 'is', 'awesome']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "corpus_encoded = label_encoder.fit_transform(words)\n",
    "corpus_encoded # какой вывод?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoder.fit_transform(corpus_encoded.reshape(-1, 1)) # какой вывод?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке для построения нейросетей [`keras`](https://keras.io/) есть более удобная функция для такого кодирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.utils\n",
    "\n",
    "keras.utils.to_categorical(corpus_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T12:05:25.824178Z",
     "start_time": "2019-01-30T12:05:25.820140Z"
    }
   },
   "source": [
    "**BoW** - \"мешок слов\". \n",
    "\n",
    "**Quiz: Что будет, если мы сложим все one-hot вектора слов в тексте?**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](BoW.jpg)\n",
    "Посчитаем количество слов в текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T11:51:36.904566Z",
     "start_time": "2019-01-30T11:51:36.901906Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Дочь бьет Марго',\n",
    "    'Тот кто бьет, не знает кто бьет его',\n",
    "    'Кто кого бьет?',\n",
    "    'Марго бьет дочь?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T11:51:44.357709Z",
     "start_time": "2019-01-30T11:51:44.350257Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Какие минусы у такого представления?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TF-IDF\n",
    "\n",
    "**Term Frequency**  $tf(w,d)$ - сколько раз слово $w$ встретилось в документе $d$\n",
    "\n",
    "**Document Frequency** $df(w)$ - сколько документов содержат слово $w$\n",
    "\n",
    "**Inverse Document Frequency** $idf(w) = log_2(N/df(w))$  — обратная документная частотность. \n",
    "\n",
    "**TF-IDF**=$tf(w,d)*idf(w)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "idf_vectorizer = TfidfVectorizer()\n",
    "idf_vectorizer.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Что это за задача? Какие ещё задачи решаются в машинном обучении?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы попробуем применить описание методы предобработки и представления текста на примере анализа тональности текста. В качестве данных будем использовать небольшой датасет твитов. Всего в данных 2 класса: позитив и негатив."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Загрузка тренировочных и тестовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Зачем нам разделять данные на тренировочные и тестовые?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим тренировочные и тестовые данные при помощи библиотеки [`pandas`](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Как оценить качество модели в задаче классификации?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th> y = 1 </th> \n",
    "    <th> y = 0 </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th> a(x) = 1 </th>\n",
    "    <td> True Positive (TP) </td> \n",
    "    <td> False Positive (FP) </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th> a(x) = 0 </th>\n",
    "    <td> False Negative (FN) </td> \n",
    "    <td> True Negative (TN) </td>\n",
    "  </tr>\n",
    "</table>\n",
    "<br>\n",
    "$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "<br><br>\n",
    "$Precision = \\frac{TP}{TP + FP}$\n",
    "<br><br>\n",
    "$Recall = \\frac{TP}{TP + FN}$\n",
    "<br><br>\n",
    "$F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для оценки векторизатора. В качестве модели будем использовать линейный SVM, он хорошо работает для определения тональности.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: Какая модель машинного обучения называется линейной? Почему мы используем линейную модель в задаче?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def evaluate_vectorizer(vectorizer):\n",
    "    X = vectorizer.fit_transform(tqdm.tqdm_notebook(train.text, desc='Vectorizing train:'))\n",
    "\n",
    "    model = LinearSVC(random_state=42)\n",
    "    model.fit(X, train.label)\n",
    "    \n",
    "    X_test = vectorizer.transform(tqdm.tqdm_notebook(test.text, desc='Vectorizing test:'))\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    print(classification_report(test.label, predictions))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Сравнение способов представления текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_vectorizer(CountVectorizer(min_df=2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_vectorizer(TfidfVectorizer(min_df=2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_vectorizer(TfidfVectorizer(min_df=2, tokenizer=tokenize_with_razdel));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=2, \n",
    "    tokenizer=lambda text: lemmatize_with_maru(tokenize_with_razdel(text)),\n",
    ")\n",
    "predictions = evaluate_vectorizer(tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix_heatmap(true, predicted):\n",
    "    classes = true.unique()\n",
    "    matrix = confusion_matrix(true, predicted, labels=classes)\n",
    "    sns.heatmap(matrix, xticklabels=classes, yticklabels=classes, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix_heatmap(test.label, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Embeddings (word2vec и друзья)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding - это векторное представление слова.\n",
    "\n",
    "| Слово  |Вектор          |\n",
    "|--------|---------------|\n",
    "| Щенок  |[0.9, 1.0, 0.0]|\n",
    "| Пёс    |[1.0, 0.2, 0.0]|\n",
    "| Котёнок |[0.0, 1.0, 0.9]|\n",
    "| Кот    |[0.0, 0.2, 1.0]|\n",
    "\n",
    "Например, в таблице первая компонента вектора эмбеддинга отражает \"собачность\" слова, вторая отвечает за \"молодость\", а третья - за \"кошачность\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T07:49:31.925225Z",
     "start_time": "2019-01-30T07:49:31.920674Z"
    }
   },
   "source": [
    "### 4.1 W2V Embeddings\n",
    "Лингвист John Firth (1957):\n",
    "> \"You shall know a word by the company it keeps\" \n",
    "> (\"Скажи мне кто друг твоего слова и скажу что это за слово\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T10:19:33.038840Z",
     "start_time": "2019-01-30T10:19:33.034585Z"
    }
   },
   "source": [
    "[T.Mikolov et al, 2013](https://arxiv.org/abs/1301.3781)\n",
    "<img src=\"w2v_context.jpeg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как такие вектора будут расположены в пространстве? \n",
    "\n",
    "> Встреча президента России Путина и президента США Обамы состоялась вчера в Кремле.\n",
    "\n",
    "> Президент США, Барак Обама, вчера уехал из Вашингтона.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Интерактивно на projector.tensorflow](http://projector.tensorflow.org/?config=https://gist.githubusercontent.com/akutuzov/fd57a53a6aeec15c3497c54bc42a9af8/raw/c29e04ee34dc7ffad8d8bcccc8da2d5905259fcc/tayga_none_fasttextcbow_300_10_2019_b7b71a84a9796c369d8566d6c64d75ee_config.json)\n",
    "\n",
    "<img src=\"w2v_pca.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Cosine: cos(\\theta) = \\frac{A \\cdot B}{\\lVert A \\rVert \\cdot \\lVert B \\rVert}$$\n",
    "\n",
    "\n",
    "$$Euclidean: d(A, B) = \\sqrt{\\sum{(A - B)^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dist.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Встречаем **Word2Vec**, модель, с помощью которой можно обучить такие эмбедденги. \n",
    "<img src=\"w2v_calc.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть 2 взаимно-обратные архитектуры Word2Vec: CBOW и Skip-Gram:   \n",
    "<img src=\"w2v_models.png\" width=\"600\">\n",
    "\n",
    "\n",
    "**Quiz: Как можно представить вектор размерностью 300 в виде вектора размерности 3?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Skip-Gram model\n",
    "\n",
    "Обучающая выборка:\n",
    "\n",
    "<img src=\"w2v_sample.png\" width=\"600\">\n",
    "\n",
    "**Quiz: В каком виде подать слово на вход нейронной сети?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель:\n",
    "\n",
    "<img src=\"w2v_skip_gram.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откуда же мы получаем эмбеддинг?\n",
    "\n",
    "<img src=\"w2v_emb_matrix.png\" width=\"500\">\n",
    "\n",
    "<img src=\"w2v_word_embedding.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения своей модели word2vec удобно использовать библиотеку [`gensim`](https://radimrehurek.com/gensim/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. И снова классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем теперь написать свой векторизатор на основе `word2vec`. Чтобы получить вектор текста, будем брать word2vec-вектора отдельных слов, суммировать в один вектор и затем этот вектор нормализовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала скачаем word2vec модель, построенную по новостным заголовкам в рамках проекта [RusVectōrēs](https://rusvectores.org/ru/models/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import gzip\n",
    "import pathlib\n",
    "import urllib.request\n",
    "\n",
    "WORD2VEC_PATH = pathlib.Path('word2vec.bin')\n",
    "\n",
    "if not WORD2VEC_PATH.exists():\n",
    "    url = 'https://rusvectores.org/static/models/rusvectores2/news_mystem_skipgram_1000_20_2015.bin.gz'\n",
    "    with urllib.request.urlopen(url) as connection:\n",
    "        compressed = connection.read()\n",
    "            \n",
    "    decompressed = gzip.GzipFile(fileobj=io.BytesIO(compressed), mode='rb').read()\n",
    "    WORD2VEC_PATH.write_bytes(decompressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы воспользоваться векторами из этой модели, также обратимся к библиотеке `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта конкретная модель обучалась не на простых токенах, а на токенах и их частеречных тегах из MyStem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec.most_similar(positive=['путин_S', 'вашингтон_S'], negative=['москва_S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала напишем функцию, которая получает частеречный разбор из MyStem и возвращает токены вида `<слово>_<часть речи>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_mystem = Mystem(entire_input=False)\n",
    "\n",
    "\n",
    "def tokenize_with_mystem_pos(text):\n",
    "    result = []\n",
    "    \n",
    "    for item in word2vec_mystem.analyze(text):\n",
    "        if item['analysis']:\n",
    "            lemma = item['analysis'][0]['lex']\n",
    "            pos = re.split('[=,]', item['analysis'][0]['gr'])[0]\n",
    "            token = f'{lemma}_{pos}'\n",
    "        else:\n",
    "            token = f'{item[\"text\"]}_UNKN'\n",
    "            \n",
    "        result.append(token)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем напишем класс, который по списку текстов возвращает вектора, полученные с помощью word2vec модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class Word2VecVectorizer(TransformerMixin):\n",
    "    def __init__(self, vectors):\n",
    "        self.vectors = vectors\n",
    "        self.zeros = np.zeros(self.vectors.vector_size)\n",
    "        \n",
    "    def _get_text_vector(self, text):\n",
    "        token_vectors = []\n",
    "        for token in tokenize_with_mystem_pos(text):\n",
    "            try:\n",
    "                token_vectors.append(self.vectors[token])\n",
    "            except KeyError: # не нашли такой токен в словаре\n",
    "                pass\n",
    "                \n",
    "        if not token_vectors:\n",
    "            return self.zeros\n",
    "\n",
    "        text_vector = np.sum(token_vectors, axis=0)\n",
    "        return text_vector / np.linalg.norm(text_vector)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([self._get_text_vector(text) for text in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_vectorizer = Word2VecVectorizer(word2vec)\n",
    "\n",
    "evaluate_vectorizer(word2vec_vectorizer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf и word2vec представления текста имеет смысл комбинировать при обучении линейных моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "evaluate_vectorizer(\n",
    "    FeatureUnion(\n",
    "        [\n",
    "            ('tf-idf', tfidf_vectorizer),\n",
    "            ('word2vec', word2vec_vectorizer),\n",
    "        ]\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Никакого~~ большого прироста в качестве мы не получили, но можно заметить, что полнота по негативу и точность по позитиву слегка выросли."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### На этом всё. А в следующий раз будет много математики. Вам понравится (¬‿¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](math.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
