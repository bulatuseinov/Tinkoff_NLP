{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "from pymystem3 import Mystem\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import string\n",
    "import razdel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с данными (1 балл)\n",
    "\n",
    "Загрузите датасет, с которым вы работали во время соревнования на kaggle. Преобразуйте его в формат, удобный для обучения модели. В качетсве фичей используйте эмбединги слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "\n",
    "def tokkenize_razdel(text):\n",
    "    return [token.text for token in razdel.tokenize(text)]\n",
    " \n",
    "def token(text):\n",
    "    result = []\n",
    "    for doc in text:    \n",
    "        doc = tokkenize_razdel(doc)\n",
    "        result.append(doc)        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SSTARTT']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokkenize_razdel('SSTARTT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "data = pd.read_csv('train.csv', index_col= False)\n",
    "\n",
    "Y = np.array(data.label)\n",
    "X = np.array(data.text)\n",
    "\n",
    "x_train_variable = \" SSTARTT \" + X\n",
    "y_train = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = 0\n",
    "oov_id = 'UNK'\n",
    "index_offset = 2\n",
    "sentence_size = 30\n",
    "vocab_size = 5000\n",
    "\n",
    "def preprocess_vectorize(data):\n",
    "    data = token(data)\n",
    "    \n",
    "    for element in data:\n",
    "        element = ' '.join(element)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words = vocab_size, \n",
    "                          filters ='!\"$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', \n",
    "                          lower=True, \n",
    "                          oov_token= oov_id)\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    data = tokenizer.texts_to_sequences(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.1 s, sys: 122 ms, total: 16.3 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_variable = preprocess_vectorize(x_train_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X, Y for train: 89973 , 89973\n",
      "X, Y for validation: 22494 , 22494\n"
     ]
    }
   ],
   "source": [
    "X_train_variable, X_val_variable, Y_train, Y_val = train_test_split(x_train_variable, y_train, test_size=0.2, random_state=42)\n",
    "print(\"X, Y for train:\", X_train.shape[0], \",\", Y_train.size)\n",
    "print(\"X, Y for validation:\", X_val.shape[0],\",\", Y_val.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train_variable, maxlen= sentence_size, padding = 'post',truncating = 'post')\n",
    "X_val = pad_sequences(X_val_variable, maxlen= sentence_size, padding = 'post',truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len_train = np.array([min(len(x), sentence_size) for x in X_train_variable])\n",
    "x_len_test = np.array([min(len(x), sentence_size) for x in X_val_variable])\n",
    "\n",
    "def parser(x, length, y):\n",
    "    features = {\"x\": x, \"len\": length}\n",
    "    return features, y\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_train, x_len_train, Y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=len(X_train_variable))\n",
    "    dataset = dataset.batch(50)\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_val, x_len_test, Y_val))\n",
    "    dataset = dataset.batch(50)\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = {}\n",
    "def train_and_evaluate(classifier):\n",
    "    # Save a reference to the classifier to run predictions later\n",
    "    all_classifiers[classifier.model_dir] = classifier\n",
    "    classifier.train(input_fn=train_input_fn, steps=25000)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    predictions = np.array([p['logistic'][0] for p in classifier.predict(input_fn=eval_input_fn)])\n",
    "        \n",
    "    # Reset the graph to be able to reuse name scopes\n",
    "    tf.reset_default_graph() \n",
    "    # Add a PR summary in addition to the summaries that the classifier writes\n",
    "    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, labels=y_test.astype(bool), num_thresholds=21)\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)\n",
    "        writer.add_summary(sess.run(pr), global_step=0)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model (1 балл)\n",
    "\n",
    "Реализуйте модель со следующими слоями, идущими в указанном порядке:\n",
    "- Conv1D, activation='relu'\n",
    "- MaxPooling1D\n",
    "- Dense\n",
    "\n",
    "Параметры модели подберите сами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = tf.feature_column.categorical_column_with_identity('x', vocab_size)\n",
    "word_embedding_column = tf.feature_column.embedding_column(column, dimension=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = tf.contrib.estimator.binary_classification_head()\n",
    "\n",
    "def cnn_model_fn(features, labels, mode, params):    \n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size, \n",
    "        initializer=params['embedding_initializer'])\n",
    "    \n",
    "    training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "#     dropout_emb = tf.layers.dropout(inputs=input_layer, \n",
    "#                                     rate=0.2, \n",
    "#                                     training=training)\n",
    "\n",
    "    conv = tf.layers.conv1d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Global Max Pooling\n",
    "    pool = tf.reduce_max(input_tensor=conv, axis=1)\n",
    "    \n",
    "    hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)\n",
    "    \n",
    "#     dropout_hidden = tf.layers.dropout(inputs=hidden, \n",
    "#                                        rate=0.2, \n",
    "#                                        training=training)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=hidden, units=1)\n",
    "    \n",
    "    # This will be None when predicting\n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        \n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    def _train_op_fn(loss):\n",
    "        return optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "\n",
    "    return head.create_estimator_spec(\n",
    "        \n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        mode=mode,\n",
    "        logits=logits, \n",
    "        train_op_fn=_train_op_fn)\n",
    "  \n",
    "params = {'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0)}\n",
    "print(\"PARAMS\" ,params)\n",
    "cnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, \n",
    "                                        model_dir=os.path.join(model_dir, 'cnn'),\n",
    "                                        params=params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_classifier.train(input_fn=train_input_fn, steps=500 )\n",
    "#eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "#prediction = classifier.predict(input_fn=eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN model (1 балл)\n",
    "\n",
    "Реализуйте модель со следующими слоями, идущими в указанном порядке:\n",
    "- RNN\n",
    "- Dense\n",
    "\n",
    "Параметры модели подберите сами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model (1 балл)\n",
    "\n",
    "Реализуйте модель со следующими слоями, идущими в указанном порядке:\n",
    "- LSTM\n",
    "- Dense\n",
    "\n",
    "Параметры модели подберите сами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните все три реализованные модели по времени и по качеству классификации. Какая лучше? Как думаете, почему?\n",
    "\n",
    "<b>Вывод:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model (2 балла)\n",
    "\n",
    "Реализуйте модель со следующими слоями, идущими в указанном порядке:\n",
    "- Conv1D, activation='relu'\n",
    "- MaxPooling1D\n",
    "- RNN\n",
    "- Dense\n",
    "\n",
    "Параметры для Conv1D, MaxPooling1D и RNN подберите сами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout (1 балл)\n",
    "\n",
    "Добавьте dropout к baseline моделе. Пример модефицированной модели:\n",
    "- Dropout\n",
    "- Conv1D, activation='relu'\n",
    "- MaxPooling1D\n",
    "- RNN\n",
    "- Dense\n",
    "\n",
    "Подберите параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшило ли результат классификации использование Dropout? Как думаете, почему?\n",
    "\n",
    "<b>Вывод:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional (1 балл)\n",
    "\n",
    "Вместо RNN в первой модели используйте biRNN. Пример модефецированной модели:\n",
    "- Conv1D, activation='relu'\n",
    "- MaxPooling1D\n",
    "- biRNN\n",
    "- Dense\n",
    "\n",
    "Подберите параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дало ли буст использовании biRNN вместо RNN? Сильный? Как думаете, почему?\n",
    "\n",
    "<b>Вывод:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom model (2 балла)\n",
    "\n",
    "Подберите архитектуру сети, используя сверточные слои и слои макспулинга и превзойдите качество полносвязной сети.\n",
    "\n",
    "Подберите архитектуру сети, используя следующие типы слоев:\n",
    "- Conv1D\n",
    "- MaxPooling1D\n",
    "- RNN\n",
    "- LSTM\n",
    "- GRU\n",
    "- Dropout\n",
    "- Dense\n",
    "\n",
    "Можно использовать любые из описанных слоев в любом количестве и в любом порядке. Не обязательно использовать все описанные слои. \n",
    "\n",
    "Настройте параметры. Превзойдите лучшее качество, полученное раннее. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
