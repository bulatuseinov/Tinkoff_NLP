{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Textual Similarity\n",
    "\n",
    "Задача семантическая близости -- это задача определения, насколько два текста похожи друг на друга по смыслу.\n",
    "\n",
    "**План на сегодня:**\n",
    "* Меры близости -- recap\n",
    "* Нормы векторов -- recap\n",
    "* Ранжирование\n",
    "* DSSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Норма вектора\n",
    "\n",
    "Норма -- это качественное измерение вектора. Обычно обозначается как $\\| x \\|$. Для нормы вектора соблюдаются следующие свойства:\n",
    "\n",
    "- $\\Vert \\alpha x \\Vert = |\\alpha| \\Vert x \\Vert$\n",
    "- $\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert$ (неравенство треугольника)\n",
    "- Если $\\Vert x \\Vert = 0$ тогда $x = 0$\n",
    "\n",
    "### Примеры\n",
    "Самая популярная норма -- **евклидова норма** ($L_2$ норма):\n",
    "\n",
    "$$\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},$$\n",
    "\n",
    "Евклидова норма является подклассом $p$-норм:\n",
    "\n",
    "$$\n",
    " \\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}.\n",
    "$$\n",
    "\n",
    "Несколько интересных случаев:\n",
    "\n",
    "- Норма Чебышева: \n",
    "\n",
    "$$\n",
    "\\Vert x \\Vert_{\\infty} = \\max_i | x_i|\n",
    "$$\n",
    "\n",
    "- $L_1$ норма (или **манхэттенское расстояние**): \n",
    "\n",
    "$$\n",
    "\\Vert x \\Vert_1 = \\sum_i |x_i|\n",
    "$$\n",
    "  \n",
    "- $L_0$ норма: $\\Vert x \\Vert_0 = $ количество ненулевых элементов.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0 norm = 100.0\n",
      "L1 norm = 49.50485324688436\n",
      "L2 norm = 5.729225789975424\n",
      "Норма Чебышева = 0.9918537686056458\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100)\n",
    "\n",
    "print('L0 norm =', np.linalg.norm(x, ord=0))\n",
    "print('L1 norm =', np.linalg.norm(x, ord=1))\n",
    "print('L2 norm =', np.linalg.norm(x, ord=2))\n",
    "print('Норма Чебышева =', np.linalg.norm(x, ord=np.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0 norm = 3.0\n",
      "L1 norm = 101.0\n",
      "L2 norm = 99.0101004948485\n",
      "Норма Чебышева = 99.0\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0, 0, 1, 1, 99])\n",
    "\n",
    "print('L0 norm =', np.linalg.norm(x, ord=0))\n",
    "print('L1 norm =', np.linalg.norm(x, ord=1))\n",
    "print('L2 norm =', np.linalg.norm(x, ord=2))\n",
    "print('Норма Чебышева =', np.linalg.norm(x, ord=np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Меры близости\n",
    "\n",
    "Мера близости — это численная мера, показывающая степень схожести двух объектов. Как правило, она выражается в виде скалярной величины.\n",
    "\n",
    "Пусть у нас есть два вектора $x$ и $y$. Как понять, насколько они похожи друг на друга?\n",
    "\n",
    "* Посчитать угол $\\theta$ между ними. $cos(\\theta) = \\frac{x^T y}{\\| x \\| \\| y \\|}$\n",
    "* Посчитать норму их разницы $\\| x - y \\|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance = 0.015414636388088382\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0, 0.001, 100)  # zero mean, and 0.001 standard deviation\n",
    "y = np.random.normal(0, 0.001, 100)\n",
    "\n",
    "print('Euclidean distance =', np.linalg.norm(x - y, ord=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance = 68.75526070902164\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0, 5, 100)  # zero mean, and 5 standard deviation\n",
    "y = np.random.normal(0, 5, 100)\n",
    "\n",
    "print('Euclidean distance =', np.linalg.norm(x - y, ord=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance = 0.0\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 1, 1])\n",
    "y = np.array([1, 1, 1])\n",
    "\n",
    "print('Euclidean distance =', np.linalg.norm(x - y, ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cv4.litres.ru/pub/c/elektronnaya-kniga/cover_415/602545-anvar-bakirov-nlp-igry-v-kotoryh-pobezhdaut-zhenschiny.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ранжирование\n",
    "\n",
    "\n",
    "Представьте, что у вас есть поисковая система, и на каждый запрос пользователя нужно выдавать правильные ссылки. Как это сделать?\n",
    "\n",
    "Как это делает [Яндекс?](https://habr.com/ru/company/yandex/blog/314222/) Как это делали наши деды?\n",
    "\n",
    "**Идея:** давайте введем меру близости и по ней будем ранжировать документы.\n",
    "\n",
    "* Hamming расстояние\n",
    "* Sørensen–Dice коэффициент\n",
    "* Jaccard индекс\n",
    "* Tversky индекс\n",
    "* Меры близости на BoW, TF-IDF представлениях\n",
    "* BM25\n",
    "\n",
    "**Jaccard:**\n",
    "\n",
    "$$\n",
    " J(A,B) = {{|A \\cap B|}\\over{|A \\cup B|}} = {{|A \\cap B|}\\over{|A| + |B| - |A \\cap B|}}.\n",
    "$$\n",
    "\n",
    "\n",
    "**BM25:**\n",
    "\n",
    "$$\n",
    "{\\text{score}}(D,Q)=\\sum _{{i=1}}^{{n}}{\\text{IDF}}(q_{i})\\cdot {\\frac  {f(q_{i},D)\\cdot (k_{1}+1)}{f(q_{i},D)+k_{1}\\cdot (1-b+b\\cdot {\\frac  {|D|}{{\\text{avgdl}}}})}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\text{IDF}}(q_{i})=\\log {\\frac  {N-n(q_{i})+0.5}{n(q_{i})+0.5}}\n",
    "$$\n",
    "\n",
    "Пусть от юзера приходит следующий запрос:\n",
    "> рассказ в котором раздавили бабочку \n",
    "\n",
    "И мы по мере близости ранжируем наши документы (пример [отсюда](https://habr.com/ru/company/yandex/blog/314222/)):\n",
    "\n",
    "\n",
    "|                  Заголовок страницы                 | BM25 | Нейронная модель |\n",
    "|:---------------------------------------------------:|:----:|:----------------:|\n",
    "| фильм в котором раздавили бабочку                   | 0.79 |             0.82 |\n",
    "| и грянул гром википедия                             |    0 |             0.43 |\n",
    "| брэдбери рэй википедия                              |    0 |             0.27 |\n",
    "| машина времени роман википедия                      |    0 |             0.24 |\n",
    "| домашнее малиновое варенье рецепт заготовки на зиму |    0 |             0.06 |\n",
    "\n",
    "\n",
    "**Вывод:** модели/подходы прошлого века не устойчивы к перефразам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jaccard\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# будем искать ближайшее предложение к первому предложению\n",
    "\n",
    "sent1 = 'добрый вечер'\n",
    "sent2 = 'добрый день'\n",
    "sent3 = 'добрейший вечерок'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag-of-words\n",
    "\n",
    "cv = CountVectorizer()\n",
    "sentences = cv.fit_transform([sent1, sent2, sent3]).toarray()\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard 0-1 = 0.6666666666666666\n",
      "Jaccard 0-2 = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Jaccard-Needham dissimilarity\n",
    "\n",
    "print('Jaccard 0-1 =', jaccard(sentences[0], sentences[1]))\n",
    "print('Jaccard 0-2 =', jaccard(sentences[0], sentences[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 59)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Использование буквенных n-грам почти всегда лучше\n",
    "# bag-of-character n-grams\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1, 3), analyzer='char')\n",
    "sentences = cv.fit_transform([sent1, sent2, sent3]).toarray()\n",
    "\n",
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard 0-1 = 0.6097560975609756\n",
      "Jaccard 0-2 = 0.5510204081632653\n"
     ]
    }
   ],
   "source": [
    "# Jaccard-Needham dissimilarity\n",
    "\n",
    "print('Jaccard 0-1 =', jaccard(sentences[0], sentences[1]))\n",
    "print('Jaccard 0-2 =', jaccard(sentences[0], sentences[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Добрейший вечерок\" стал ближе, чем \"добрый день\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSSM and Beyond\n",
    "\n",
    "А что NLP? Оказывается, подобным способом можно классифицировать тексты и делать многое другое, но сначала давайте познакомимся с идеями статьи [Learning Deep Structured Semantic Models for Web Search using Clickthrough Data](https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/) от Microsoft.\n",
    "\n",
    "Ребята из Microsoft улучшали свой поисковик. В качестве данных у них были запросы пользователей и соответственные клики (не клики) на ссылки. \n",
    "\n",
    "### Model\n",
    "Обозначим за $Q_i$ -- запрос пользователя и $D_j$ -- документ (веб-страницу).\n",
    "\n",
    "Выходы моделей обозначим за $M_Q(Q_i)$ и $M_D(D_j)$. Наша цель -- обучить модель так, чтобы расстояние от запроса пользователя до правильного ответа (документа), было меньше, чем расстояние до неправильного.\n",
    "\n",
    "$$\n",
    "dist(M_Q(Q_i), M_D(D_i^+)) < dist(M_Q(Q_i), M_D(D_i^-))\n",
    "$$\n",
    "\n",
    "Мы собираем наборы из $(Q, D^+, D_1^-, D_2^-, \\cdots, D_k^-)$ и проганяем их через следующую сеть:\n",
    "<img src=\"https://raw.githubusercontent.com/v-liaha/v-liaha.github.io/master/assets/dssm.png\" width=\"600\">\n",
    "\n",
    "В качестве функции потерь мы можем использовать:\n",
    "\n",
    "$$\n",
    "L(\\Lambda) = - \\log \\prod_{Q, D^+} P(D^+|Q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическая часть\n",
    "\n",
    "Попробуем перенести наши идеи на TensorFlow.\n",
    "\n",
    "<img src='https://scontent.fhrk1-1.fna.fbcdn.net/v/t1.0-9/37006042_518786041909234_5582352793441665024_n.png?_nc_cat=104&oh=8e36fa4907bb5f15c621c4c9b9215bc8&oe=5C8960E4' width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем данные [Quora Question Pairs](https://www.kaggle.com/quora/question-pairs-dataset)\n",
    "\n",
    "**Описание данных:**\n",
    "\n",
    "* id - the id of a training set question pair\n",
    "* qid1, qid2 - unique ids of each question (only available in train.csv)\n",
    "* question1, question2 - the full text of each question\n",
    "* is_duplicate - the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`’:]\", \" \", string)  \n",
    "    string = re.sub(r\"’\", \"'\", string) \n",
    "    string = re.sub(r\"`\", \"'\", string) \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\":\", \" : \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" ( \", string) \n",
    "    string = re.sub(r\"\\)\", \" ) \", string) \n",
    "    string = re.sub(r\"\\?\", \" ? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('questions.csv', nrows=50000)\n",
    "data = data[data['is_duplicate'] == 1]\n",
    "data = data.dropna()\n",
    "data = data.rename({'question1': 'query', 'question2': 'd+'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>query</th>\n",
       "      <th>d+</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>How can I see all my Youtube comments?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  qid1  qid2                                              query  \\\n",
       "5    5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "7    7    15    16                     How can I be a good geologist?   \n",
       "11  11    23    24        How do I read and find my YouTube comments?   \n",
       "12  12    25    26               What can make Physics easy to learn?   \n",
       "13  13    27    28        What was your first sexual experience like?   \n",
       "\n",
       "                                                   d+  is_duplicate  \n",
       "5   I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "7           What should I do to be a great geologist?             1  \n",
       "11             How can I see all my Youtube comments?             1  \n",
       "12            How can you make physics easy to learn?             1  \n",
       "13             What was your first sexual experience?             1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negatives = 5\n",
    "\n",
    "data['query'] = data['query'].apply(lambda x: tokenize_string(x))\n",
    "data['d+'] = data['d+'].apply(lambda x: tokenize_string(x))\n",
    "\n",
    "for i in range(num_negatives):\n",
    "    data[f'd{i}-'] = np.random.permutation(data['d+'].values)\n",
    "\n",
    "y = np.zeros((data.shape[0], num_negatives + 1), dtype=np.int64)\n",
    "y[:, 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: почему так собирать негативные примеры плохо?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>d+</th>\n",
       "      <th>d0-</th>\n",
       "      <th>d1-</th>\n",
       "      <th>d2-</th>\n",
       "      <th>d3-</th>\n",
       "      <th>d4-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10356</th>\n",
       "      <td>why does my 5 month old boxer puppy bark all n...</td>\n",
       "      <td>how do you stop a puppy from barking all night ?</td>\n",
       "      <td>how can india become cashless ?</td>\n",
       "      <td>what are some of the best contemporary books ?</td>\n",
       "      <td>what does a donald trump presidency mean for t...</td>\n",
       "      <td>how does google 's new nocaptcha recaptcha work ?</td>\n",
       "      <td>what will happen when the andromeda galaxy col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48759</th>\n",
       "      <td>how is flirting healthy ?</td>\n",
       "      <td>is flirting good for health ?</td>\n",
       "      <td>who is the best escort service provider in ban...</td>\n",
       "      <td>can i use jio sim in 3g handset phone ?</td>\n",
       "      <td>what are the differences between chinese and w...</td>\n",
       "      <td>what were the major effects of the cambodia ea...</td>\n",
       "      <td>what are some shoes similar to chuck taylor sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42883</th>\n",
       "      <td>how big do you think the vr sector will be ?</td>\n",
       "      <td>how big will vr be ?</td>\n",
       "      <td>why did n't the ina get integrated in the indi...</td>\n",
       "      <td>what are your top three favorite books and why ?</td>\n",
       "      <td>can i substitute white sugar for brown ?</td>\n",
       "      <td>can meth be out of your system in 48 hours ?</td>\n",
       "      <td>what countries did yugoslavia split into ? whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38139</th>\n",
       "      <td>are dark matter and neutrinos at some level th...</td>\n",
       "      <td>are neutrinos dark matter ?</td>\n",
       "      <td>why ms dhoni retire from test ?</td>\n",
       "      <td>when and how did time begin ?</td>\n",
       "      <td>what are the best places to visit in india ?</td>\n",
       "      <td>jio bar code lost re generated ?</td>\n",
       "      <td>how it is like to be a pornstar ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32194</th>\n",
       "      <td>why do tall guys love short girls ?</td>\n",
       "      <td>why do tall guys like short ladies ?</td>\n",
       "      <td>what are some things you should n't do or say ...</td>\n",
       "      <td>how can i overcome the fear of failure ?</td>\n",
       "      <td>how can i lose weight ?</td>\n",
       "      <td>what are the principles of buddhism ?</td>\n",
       "      <td>what is the secret of keeping a successful lon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   query  \\\n",
       "10356  why does my 5 month old boxer puppy bark all n...   \n",
       "48759                          how is flirting healthy ?   \n",
       "42883       how big do you think the vr sector will be ?   \n",
       "38139  are dark matter and neutrinos at some level th...   \n",
       "32194                why do tall guys love short girls ?   \n",
       "\n",
       "                                                     d+  \\\n",
       "10356  how do you stop a puppy from barking all night ?   \n",
       "48759                     is flirting good for health ?   \n",
       "42883                              how big will vr be ?   \n",
       "38139                       are neutrinos dark matter ?   \n",
       "32194              why do tall guys like short ladies ?   \n",
       "\n",
       "                                                     d0-  \\\n",
       "10356                    how can india become cashless ?   \n",
       "48759  who is the best escort service provider in ban...   \n",
       "42883  why did n't the ina get integrated in the indi...   \n",
       "38139                    why ms dhoni retire from test ?   \n",
       "32194  what are some things you should n't do or say ...   \n",
       "\n",
       "                                                    d1-  \\\n",
       "10356    what are some of the best contemporary books ?   \n",
       "48759           can i use jio sim in 3g handset phone ?   \n",
       "42883  what are your top three favorite books and why ?   \n",
       "38139                     when and how did time begin ?   \n",
       "32194          how can i overcome the fear of failure ?   \n",
       "\n",
       "                                                     d2-  \\\n",
       "10356  what does a donald trump presidency mean for t...   \n",
       "48759  what are the differences between chinese and w...   \n",
       "42883           can i substitute white sugar for brown ?   \n",
       "38139       what are the best places to visit in india ?   \n",
       "32194                            how can i lose weight ?   \n",
       "\n",
       "                                                     d3-  \\\n",
       "10356  how does google 's new nocaptcha recaptcha work ?   \n",
       "48759  what were the major effects of the cambodia ea...   \n",
       "42883       can meth be out of your system in 48 hours ?   \n",
       "38139                   jio bar code lost re generated ?   \n",
       "32194              what are the principles of buddhism ?   \n",
       "\n",
       "                                                     d4-  \n",
       "10356  what will happen when the andromeda galaxy col...  \n",
       "48759  what are some shoes similar to chuck taylor sh...  \n",
       "42883  what countries did yugoslavia split into ? whi...  \n",
       "38139                  how it is like to be a pornstar ?  \n",
       "32194  what is the secret of keeping a successful lon...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['query', 'd+'] + [f'd{i}-' for i in range(num_negatives)]\n",
    "\n",
    "data[columns].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# фитим токенайзер\n",
    "\n",
    "time_steps = 15\n",
    "vocab_size = 5000\n",
    "\n",
    "corpus = data['query'].tolist() + data['d+'].tolist()\n",
    "tok = Tokenizer(num_words=vocab_size)\n",
    "tok.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.18 s, sys: 4.9 ms, total: 2.19 s\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def vectorize(tokenizer, data, time_steps):\n",
    "    data = tokenizer.texts_to_sequences(data)\n",
    "    data = pad_sequences(data, maxlen=time_steps, padding='post')\n",
    "    return data\n",
    "\n",
    "X = {col: vectorize(tok, data[col].values, time_steps) for col in columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18650, 15), (18650, 15))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['query'].shape, X['d+'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем отложенную выборку\n",
    "\n",
    "ind_train, ind_eval = train_test_split(np.arange(len(data)), test_size=0.1, random_state=24)\n",
    "\n",
    "X_train = {key: val[ind_train] for key, val in X.items()}\n",
    "X_eval = {key: val[ind_eval] for key, val in X.items()}\n",
    "\n",
    "y_train = y[ind_train]\n",
    "y_eval = y[ind_eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подача данных в модель\n",
    "\n",
    "def input_fn(x, labels, params, is_training):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, labels))\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=params['buffer_size'])\n",
    "        dataset = dataset.repeat(count=params['num_epochs'])\n",
    "\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.prefetch(buffer_size=2)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://pp.userapi.com/c845018/v845018002/115e82/MAdbYTyJXnA.jpg' width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x, y):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity\n",
    "    Args:\n",
    "        x: [batch_size, emb_dim]\n",
    "        y: [batch_size, emb_dim]\n",
    "    Returns:\n",
    "        cos_sim: tf.float64\n",
    "    \"\"\"\n",
    "    xn = tf.nn.l2_normalize(x, axis=1)\n",
    "    yn = tf.nn.l2_normalize(y, axis=1)\n",
    "    cos_sim = tf.expand_dims(tf.reduce_sum(tf.multiply(xn, yn), axis=1), axis=-1)\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем архитектуру\n",
    "\n",
    "def build_model(features, params, is_training):\n",
    "    \n",
    "    # Embedding matrix\n",
    "    emb_matrix = tf.get_variable('embedding_matrix',\n",
    "                                 shape=[params['vocab_size'], params['emb_size']],\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    # Our architecture (try CNN/RNN/Whatever you want)\n",
    "    def encode(x):\n",
    "        embs = tf.nn.embedding_lookup(emb_matrix, x)\n",
    "        max_pool = tf.reduce_max(embs, axis=1)\n",
    "            \n",
    "        with tf.name_scope('dense'):\n",
    "            out = tf.layers.dense(max_pool, 64)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    # the same architecture for the query and documents\n",
    "    encoded_features = {}        \n",
    "    \n",
    "    with tf.variable_scope('encoder'):\n",
    "        encoded_features['query'] = encode(features['query'])\n",
    "    \n",
    "    for key, value in features.items():\n",
    "        if key != 'query':\n",
    "            with tf.variable_scope('encoder', reuse=True):\n",
    "                encoded_features[key] = encode(value)\n",
    "    \n",
    "    # Calculating cosine similarities\n",
    "    cos_sims = {}\n",
    "    for key, value in encoded_features.items():\n",
    "        if key != 'query':\n",
    "            cos_sims[key] = cosine_sim(encoded_features['query'], encoded_features[key])\n",
    "    \n",
    "    cosine_similarities = [cos_sims['d+']]\n",
    "    for k in range(len(features) - 2):\n",
    "        cosine_similarities.append(cos_sims[f'd{k}-'])\n",
    "\n",
    "    cosine_similarities = tf.concat(cosine_similarities, axis=1)\n",
    "    \n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем: оптимайзер, лосс, метрики, поведение модели в зависимости от mode\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    with tf.variable_scope('model'):\n",
    "        logits = build_model(features, params, is_training)\n",
    "        \n",
    "    preds = tf.argmax(logits, axis=1)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {'preds': preds}\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions)\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, tf.constant(0, dtype=tf.int64)), tf.float64))\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        with tf.variable_scope('metrics'):\n",
    "            eval_metrics = {'accuracy': tf.metrics.mean(accuracy)}\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metrics)\n",
    "    \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    global_step = tf.train.get_global_step()\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model_params = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'emb_size': 150\n",
    "}\n",
    "\n",
    "config = tf.estimator.RunConfig(tf_random_seed=123,\n",
    "                                model_dir='experiment_0',\n",
    "                                save_summary_steps=5)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn,\n",
    "                                   params=model_params,\n",
    "                                   config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 128,\n",
    "    'num_epochs': 5,\n",
    "    'buffer_size': int(len(data) * 0.3)\n",
    "}\n",
    "\n",
    "estimator.train(lambda: input_fn(X_train, y_train, params=params, is_training=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_results = estimator.evaluate(lambda: input_fn(X_eval, y_eval, params=params, is_training=False))\n",
    "\n",
    "for key, value in eval_results.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** мы с большой точностью умеем среди $k$ вопросов находить тот, который является дубликатом к исходному."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss\n",
    "\n",
    "Для metric learning (similarity learning) больше подходит другой лосс, с которым мы сегодня познакомимся.\n",
    "\n",
    "**Идея:** давайте придумаем такой лосс, который будет притягивать примеры из одного класса и отталкивать примеры из разных классов. Для этого из батча будет формировать триплеты: anchor, positive, negative (A, P, N), такие что\n",
    "\n",
    "* A и P из одного класса\n",
    "* A и N из разных классов\n",
    "\n",
    "Тогда функция потерь выглядит следующим образом:\n",
    "\n",
    "$$\n",
    "L = \\frac 1N \\underset {Q, D^+, D^-} \\sum max(0, \\space m - sim[M_Q(Q), M_D(D^+)] + sim[M_Q(Q), M_D(D^-)] )\n",
    "$$\n",
    "\n",
    "**Интерпретация:** правильные документы должны быть ближе, чем неправильные, как минимум на величину *margin* -- $m$\n",
    "\n",
    "Заметьте, модели для запросов пользователей и документов могут быть одинаковыми $M_Q = M_D$ (одинаковые веса)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До и после обучения:\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/0*_WNBFcRVEOz6QM7R.' width=\"400\">\n",
    "\n",
    "На примере задаче Question Answering:\n",
    "<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/margin.png' width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стратегии обучения\n",
    "\n",
    "Стратегии формирования триплетов:\n",
    "* Offline triplet mining (формируем триплеты каждую эпоху)\n",
    "* Online triplet mining (формируем триплеты каждый батч)\n",
    "\n",
    "Стратегии обучения для online mining:\n",
    "* Batch all strategy (все примеры могут пойти в триплеты)\n",
    "* Batch hard strategy (в триплеты идут только самые сложные примеры)\n",
    "\n",
    "<img src='https://omoindrot.github.io/assets/triplet_loss/triplets.png' width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наивная реализация триплет лосса:\n",
    "\n",
    "```python\n",
    "anchor_output = ...    # shape [None, 128]\n",
    "positive_output = ...  # shape [None, 128]\n",
    "negative_output = ...  # shape [None, 128]\n",
    "\n",
    "d_pos = tf.reduce_sum(tf.square(anchor_output - positive_output), 1)\n",
    "d_neg = tf.reduce_sum(tf.square(anchor_output - negative_output), 1)\n",
    "\n",
    "loss = tf.maximum(0.0, margin + d_pos - d_neg)\n",
    "loss = tf.reduce_mean(loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Хорошо, а как предсказывать? С помощью kNN!\n",
    "\n",
    "<img src='https://pbs.twimg.com/media/DmVRIqrXcAAOvtH.jpg' width=\"400\">\n",
    "\n",
    "Плюсы kNN:\n",
    "1. Существуют [множество](https://github.com/erikbern/ann-benchmarks) методов, которые позволяют искать очень быстро.\n",
    "2. Качество на train set'е близко к 100%\n",
    "3. Вы можете легко включать и выключать любые классы из предсказаний"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как реализовать на TensorFlow?\n",
    "\n",
    "В contrib ветке TensorFlow лежит реализованный [triplet loss](https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/contrib/losses/metric_learning/triplet_semihard_loss). Попробуйте применить его на своей задаче!\n",
    "\n",
    "Или посмотрите на [эту](https://github.com/omoindrot/tensorflow-triplet-loss) реализацию triplet loss'a с нуля"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры задач\n",
    "\n",
    "* Text classification\n",
    "* Face identification\n",
    "* Chitchat\n",
    "* Image captioning\n",
    "* Твоя идея?\n",
    "\n",
    "<img src='https://omoindrot.github.io/assets/triplet_loss/triplet_loss.png' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papers/blog posts\n",
    "\n",
    "* [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/abs/1503.03832)\n",
    "* [Siamese Neural Networks for One-shot Image Recognition](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "* [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/abs/1703.07737)\n",
    "* [OpenFace 0.2.0: Higher accuracy and halved execution time](http://bamos.github.io/2016/01/19/openface-0.2.0/)\n",
    "* [Triplet Loss and Online Triplet Mining in TensorFlow](https://omoindrot.github.io/triplet-loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопросы:\n",
    "\n",
    "* В чем минусы триплет лосса? как исправить?\n",
    "* В чем преимущество triplet loss'а на задаче классификации по сравнению с log-loss'ом, если классов миллиард?\n",
    "* Сколько всего комбинаций триплетов в батче?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
