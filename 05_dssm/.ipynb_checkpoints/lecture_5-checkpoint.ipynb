{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Textual Similarity\n",
    "\n",
    "Задача семантическая близости -- это задача определения, насколько два текста похожи друг на друга по смыслу.\n",
    "\n",
    "**План на сегодня:**\n",
    "* Меры близости -- recap\n",
    "* Нормы векторов -- recap\n",
    "* Ранжирование\n",
    "* DSSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Норма вектора\n",
    "\n",
    "Норма -- это качественное измерение вектора. Обычно обозначается как $\\| x \\|$. Для нормы вектора соблюдаются следующие свойства:\n",
    "\n",
    "- $\\Vert \\alpha x \\Vert = |\\alpha| \\Vert x \\Vert$\n",
    "- $\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert$ (неравенство треугольника)\n",
    "- Если $\\Vert x \\Vert = 0$ тогда $x = 0$\n",
    "\n",
    "### Примеры\n",
    "Самая популярная норма -- **евклидова норма** ($L_2$ норма):\n",
    "\n",
    "$$\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},$$\n",
    "\n",
    "Евклидова норма является подклассом $p$-норм:\n",
    "\n",
    "$$\n",
    " \\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}.\n",
    "$$\n",
    "\n",
    "Несколько интересных случаев:\n",
    "\n",
    "- Норма Чебышева: \n",
    "\n",
    "$$\n",
    "\\Vert x \\Vert_{\\infty} = \\max_i | x_i|\n",
    "$$\n",
    "\n",
    "- $L_1$ норма (или **манхэттенское расстояние**): \n",
    "\n",
    "$$\n",
    "\\Vert x \\Vert_1 = \\sum_i |x_i|\n",
    "$$\n",
    "  \n",
    "- $L_0$ норма: $\\Vert x \\Vert_0 = $ количество ненулевых элементов.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0 norm = 100.0\n",
      "L1 norm = 53.39669757118147\n",
      "L2 norm = 6.106156477617053\n",
      "Норма Чебышева = 0.9980021649681315\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100)\n",
    "\n",
    "print('L0 norm =', np.linalg.norm(x, ord=0))\n",
    "print('L1 norm =', np.linalg.norm(x, ord=1))\n",
    "print('L2 norm =', np.linalg.norm(x, ord=2))\n",
    "print('Норма Чебышева =', np.linalg.norm(x, ord=np.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0 norm = 3.0\n",
      "L1 norm = 101.0\n",
      "L2 norm = 99.0101004948485\n",
      "Норма Чебышева = 99.0\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0, 0, 1, 1, 99])\n",
    "\n",
    "print('L0 norm =', np.linalg.norm(x, ord=0))\n",
    "print('L1 norm =', np.linalg.norm(x, ord=1))\n",
    "print('L2 norm =', np.linalg.norm(x, ord=2))\n",
    "print('Норма Чебышева =', np.linalg.norm(x, ord=np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Меры близости\n",
    "\n",
    "Мера близости — это численная мера, показывающая степень схожести двух объектов. Как правило, она выражается в виде скалярной величины.\n",
    "\n",
    "Пусть у нас есть два вектора $x$ и $y$. Как понять, насколько они похожи друг на друга?\n",
    "\n",
    "* Посчитать угол $\\theta$ между ними. $cos(\\theta) = \\frac{x^T y}{\\| x \\| \\| y \\|}$\n",
    "* Посчитать норму их разницы $\\| x - y \\|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance = 0.014925225909625337\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0, 0.001, 100)  # zero mean, and 0.001 standard deviation\n",
    "y = np.random.normal(0, 0.001, 100)\n",
    "\n",
    "print('Euclidean distance =', np.linalg.norm(x - y, ord=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance = 65.29305419017898\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0, 5, 100)  # zero mean, and 5 standard deviation\n",
    "y = np.random.normal(0, 5, 100)\n",
    "\n",
    "print('Euclidean distance =', np.linalg.norm(x - y, ord=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance = 0.0\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 1, 1])\n",
    "y = np.array([1, 1, 1])\n",
    "\n",
    "print('Euclidean distance =', np.linalg.norm(x - y, ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cv4.litres.ru/pub/c/elektronnaya-kniga/cover_415/602545-anvar-bakirov-nlp-igry-v-kotoryh-pobezhdaut-zhenschiny.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ранжирование\n",
    "\n",
    "\n",
    "Представьте, что у вас есть поисковая система, и на каждый запрос пользователя нужно выдавать правильные ссылки. Как это сделать?\n",
    "\n",
    "Как это делает [Яндекс?](https://habr.com/ru/company/yandex/blog/314222/) Как это делали наши деды?\n",
    "\n",
    "**Идея:** давайте введем меру близости и по ней будем ранжировать документы.\n",
    "\n",
    "* Hamming расстояние\n",
    "* Sørensen–Dice коэффициент\n",
    "* Jaccard индекс\n",
    "* Tversky индекс\n",
    "* Меры близости на BoW, TF-IDF представлениях\n",
    "* BM25\n",
    "\n",
    "**Jaccard:**\n",
    "\n",
    "$$\n",
    " J(A,B) = {{|A \\cap B|}\\over{|A \\cup B|}} = {{|A \\cap B|}\\over{|A| + |B| - |A \\cap B|}}.\n",
    "$$\n",
    "\n",
    "\n",
    "**BM25:**\n",
    "\n",
    "$$\n",
    "{\\text{score}}(D,Q)=\\sum _{{i=1}}^{{n}}{\\text{IDF}}(q_{i})\\cdot {\\frac  {f(q_{i},D)\\cdot (k_{1}+1)}{f(q_{i},D)+k_{1}\\cdot (1-b+b\\cdot {\\frac  {|D|}{{\\text{avgdl}}}})}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\text{IDF}}(q_{i})=\\log {\\frac  {N-n(q_{i})+0.5}{n(q_{i})+0.5}}\n",
    "$$\n",
    "\n",
    "Пусть от юзера приходит следующий запрос:\n",
    "> рассказ в котором раздавили бабочку \n",
    "\n",
    "И мы по мере близости ранжируем наши документы (пример [отсюда](https://habr.com/ru/company/yandex/blog/314222/)):\n",
    "\n",
    "\n",
    "|                  Заголовок страницы                 | BM25 | Нейронная модель |\n",
    "|:---------------------------------------------------:|:----:|:----------------:|\n",
    "| фильм в котором раздавили бабочку                   | 0.79 |             0.82 |\n",
    "| и грянул гром википедия                             |    0 |             0.43 |\n",
    "| брэдбери рэй википедия                              |    0 |             0.27 |\n",
    "| машина времени роман википедия                      |    0 |             0.24 |\n",
    "| домашнее малиновое варенье рецепт заготовки на зиму |    0 |             0.06 |\n",
    "\n",
    "\n",
    "**Вывод:** модели/подходы прошлого века не устойчивы к перефразам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jaccard\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# будем искать ближайшее предложение к первому предложению\n",
    "\n",
    "sent1 = 'добрый вечер'\n",
    "sent2 = 'добрый день'\n",
    "sent3 = 'добрейший вечерок'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag-of-words\n",
    "\n",
    "cv = CountVectorizer()\n",
    "sentences = cv.fit_transform([sent1, sent2, sent3]).toarray()\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard 0-1 = 0.6666666666666666\n",
      "Jaccard 0-2 = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Jaccard-Needham dissimilarity\n",
    "\n",
    "print('Jaccard 0-1 =', jaccard(sentences[0], sentences[1]))\n",
    "print('Jaccard 0-2 =', jaccard(sentences[0], sentences[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 59)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Использование буквенных n-грам почти всегда лучше\n",
    "# bag-of-character n-grams\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1, 3), analyzer='char')\n",
    "sentences = cv.fit_transform([sent1, sent2, sent3]).toarray()\n",
    "\n",
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard 0-1 = 0.6097560975609756\n",
      "Jaccard 0-2 = 0.5510204081632653\n"
     ]
    }
   ],
   "source": [
    "# Jaccard-Needham dissimilarity\n",
    "\n",
    "print('Jaccard 0-1 =', jaccard(sentences[0], sentences[1]))\n",
    "print('Jaccard 0-2 =', jaccard(sentences[0], sentences[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Добрейший вечерок\" стал ближе, чем \"добрый день\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSSM and Beyond\n",
    "\n",
    "А что NLP? Оказывается, подобным способом можно классифицировать тексты и делать многое другое, но сначала давайте познакомимся с идеями статьи [Learning Deep Structured Semantic Models for Web Search using Clickthrough Data](https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/) от Microsoft.\n",
    "\n",
    "Ребята из Microsoft улучшали свой поисковик. В качестве данных у них были запросы пользователей и соответственные клики (не клики) на ссылки. \n",
    "\n",
    "### Model\n",
    "Обозначим за $Q_i$ -- запрос пользователя и $D_j$ -- документ (веб-страницу).\n",
    "\n",
    "Выходы моделей обозначим за $M_Q(Q_i)$ и $M_D(D_j)$. Наша цель -- обучить модель так, чтобы расстояние от запроса пользователя до правильного ответа (документа), было меньше, чем расстояние до неправильного.\n",
    "\n",
    "$$\n",
    "dist(M_Q(Q_i), M_D(D_i^+)) < dist(M_Q(Q_i), M_D(D_i^-))\n",
    "$$\n",
    "\n",
    "Мы собираем наборы из $(Q, D^+, D_1^-, D_2^-, \\cdots, D_k^-)$ и проганяем их через следующую сеть:\n",
    "<img src=\"https://raw.githubusercontent.com/v-liaha/v-liaha.github.io/master/assets/dssm.png\" width=\"600\">\n",
    "\n",
    "В качестве функции потерь мы можем использовать:\n",
    "\n",
    "$$\n",
    "L(\\Lambda) = - \\log \\prod_{Q, D^+} P(D^+|Q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическая часть\n",
    "\n",
    "Попробуем перенести наши идеи на TensorFlow.\n",
    "\n",
    "<img src='https://scontent.fhrk1-1.fna.fbcdn.net/v/t1.0-9/37006042_518786041909234_5582352793441665024_n.png?_nc_cat=104&oh=8e36fa4907bb5f15c621c4c9b9215bc8&oe=5C8960E4' width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем данные [Quora Question Pairs](https://www.kaggle.com/quora/question-pairs-dataset)\n",
    "\n",
    "**Описание данных:**\n",
    "\n",
    "* id - the id of a training set question pair\n",
    "* qid1, qid2 - unique ids of each question (only available in train.csv)\n",
    "* question1, question2 - the full text of each question\n",
    "* is_duplicate - the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_string(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`’:]\", \" \", string)  \n",
    "    string = re.sub(r\"’\", \"'\", string) \n",
    "    string = re.sub(r\"`\", \"'\", string) \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\":\", \" : \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" ( \", string) \n",
    "    string = re.sub(r\"\\)\", \" ) \", string) \n",
    "    string = re.sub(r\"\\?\", \" ? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('questions.csv', nrows=50000)\n",
    "data = data[data['is_duplicate'] == 1]\n",
    "data = data.dropna()\n",
    "data = data.rename({'question1': 'query', 'question2': 'd+'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_negatives = 5\n",
    "\n",
    "data['query'] = data['query'].apply(lambda x: tokenize_string(x))\n",
    "data['d+'] = data['d+'].apply(lambda x: tokenize_string(x))\n",
    "\n",
    "for i in range(num_negatives):\n",
    "    data[f'd{i}-'] = np.random.permutation(data['d+'].values)\n",
    "\n",
    "y = np.zeros((data.shape[0], num_negatives + 1), dtype=np.int64)\n",
    "y[:, 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: почему так собирать негативные примеры плохо?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['query', 'd+'] + [f'd{i}-' for i in range(num_negatives)]\n",
    "\n",
    "data[columns].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# фитим токенайзер\n",
    "\n",
    "time_steps = 15\n",
    "vocab_size = 5000\n",
    "\n",
    "corpus = data['query'].tolist() + data['d+'].tolist()\n",
    "tok = Tokenizer(num_words=vocab_size)\n",
    "tok.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def vectorize(tokenizer, data, time_steps):\n",
    "    data = tokenizer.texts_to_sequences(data)\n",
    "    data = pad_sequences(data, maxlen=time_steps, padding='post')\n",
    "    return data\n",
    "\n",
    "X = {col: vectorize(tok, data[col].values, time_steps) for col in columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X['query'].shape, X['d+'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# создаем отложенную выборку\n",
    "\n",
    "ind_train, ind_eval = train_test_split(np.arange(len(data)), test_size=0.1, random_state=24)\n",
    "\n",
    "X_train = {key: val[ind_train] for key, val in X.items()}\n",
    "X_eval = {key: val[ind_eval] for key, val in X.items()}\n",
    "\n",
    "y_train = y[ind_train]\n",
    "y_eval = y[ind_eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# подача данных в модель\n",
    "\n",
    "def input_fn(x, labels, params, is_training):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, labels))\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=params['buffer_size'])\n",
    "        dataset = dataset.repeat(count=params['num_epochs'])\n",
    "\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.prefetch(buffer_size=2)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://pp.userapi.com/c845018/v845018002/115e82/MAdbYTyJXnA.jpg' width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(x, y):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity\n",
    "    Args:\n",
    "        x: [batch_size, emb_dim]\n",
    "        y: [batch_size, emb_dim]\n",
    "    Returns:\n",
    "        cos_sim: tf.float64\n",
    "    \"\"\"\n",
    "    xn = tf.nn.l2_normalize(x, axis=1)\n",
    "    yn = tf.nn.l2_normalize(y, axis=1)\n",
    "    cos_sim = tf.expand_dims(tf.reduce_sum(tf.multiply(xn, yn), axis=1), axis=-1)\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# определяем архитектуру\n",
    "\n",
    "def build_model(features, params, is_training):\n",
    "    \n",
    "    # Embedding matrix\n",
    "    emb_matrix = tf.get_variable('embedding_matrix',\n",
    "                                 shape=[params['vocab_size'], params['emb_size']],\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    # Our architecture (try CNN/RNN/Whatever you want)\n",
    "    def encode(x):\n",
    "        embs = tf.nn.embedding_lookup(emb_matrix, x)\n",
    "        max_pool = tf.reduce_max(embs, axis=1)\n",
    "            \n",
    "        with tf.name_scope('dense'):\n",
    "            out = tf.layers.dense(max_pool, 64)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    # the same architecture for the query and documents\n",
    "    encoded_features = {}        \n",
    "    \n",
    "    with tf.variable_scope('encoder'):\n",
    "        encoded_features['query'] = encode(features['query'])\n",
    "    \n",
    "    for key, value in features.items():\n",
    "        if key != 'query':\n",
    "            with tf.variable_scope('encoder', reuse=True):\n",
    "                encoded_features[key] = encode(value)\n",
    "    \n",
    "    # Calculating cosine similarities\n",
    "    cos_sims = {}\n",
    "    for key, value in encoded_features.items():\n",
    "        if key != 'query':\n",
    "            cos_sims[key] = cosine_sim(encoded_features['query'], encoded_features[key])\n",
    "    \n",
    "    cosine_similarities = [cos_sims['d+']]\n",
    "    for k in range(len(features) - 2):\n",
    "        cosine_similarities.append(cos_sims[f'd{k}-'])\n",
    "\n",
    "    cosine_similarities = tf.concat(cosine_similarities, axis=1)\n",
    "    \n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# определяем: оптимайзер, лосс, метрики, поведение модели в зависимости от mode\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    with tf.variable_scope('model'):\n",
    "        logits = build_model(features, params, is_training)\n",
    "        \n",
    "    preds = tf.argmax(logits, axis=1)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {'preds': preds}\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions)\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, tf.constant(0, dtype=tf.int64)), tf.float64))\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        with tf.variable_scope('metrics'):\n",
    "            eval_metrics = {'accuracy': tf.metrics.mean(accuracy)}\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metrics)\n",
    "    \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    global_step = tf.train.get_global_step()\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model_params = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'emb_size': 150\n",
    "}\n",
    "\n",
    "config = tf.estimator.RunConfig(tf_random_seed=123,\n",
    "                                model_dir='experiment_0',\n",
    "                                save_summary_steps=5)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn,\n",
    "                                   params=model_params,\n",
    "                                   config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 128,\n",
    "    'num_epochs': 5,\n",
    "    'buffer_size': int(len(data) * 0.3)\n",
    "}\n",
    "\n",
    "estimator.train(lambda: input_fn(X_train, y_train, params=params, is_training=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_results = estimator.evaluate(lambda: input_fn(X_eval, y_eval, params=params, is_training=False))\n",
    "\n",
    "for key, value in eval_results.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** мы с большой точностью умеем среди $k$ вопросов находить тот, который является дубликатом к исходному."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss\n",
    "\n",
    "Для metric learning (similarity learning) больше подходит другой лосс, с которым мы сегодня познакомимся.\n",
    "\n",
    "**Идея:** давайте придумаем такой лосс, который будет притягивать примеры из одного класса и отталкивать примеры из разных классов. Для этого из батча будет формировать триплеты: anchor, positive, negative (A, P, N), такие что\n",
    "\n",
    "* A и P из одного класса\n",
    "* A и N из разных классов\n",
    "\n",
    "Тогда функция потерь выглядит следующим образом:\n",
    "\n",
    "$$\n",
    "L = \\frac 1N \\underset {Q, D^+, D^-} \\sum max(0, \\space m - sim[M_Q(Q), M_D(D^+)] + sim[M_Q(Q), M_D(D^-)] )\n",
    "$$\n",
    "\n",
    "**Интерпретация:** правильные документы должны быть ближе, чем неправильные, как минимум на величину *margin* -- $m$\n",
    "\n",
    "Заметьте, модели для запросов пользователей и документов могут быть одинаковыми $M_Q = M_D$ (одинаковые веса)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До и после обучения:\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/0*_WNBFcRVEOz6QM7R.' width=\"400\">\n",
    "\n",
    "На примере задаче Question Answering:\n",
    "<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/margin.png' width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стратегии обучения\n",
    "\n",
    "Стратегии формирования триплетов:\n",
    "* Offline triplet mining (формируем триплеты каждую эпоху)\n",
    "* Online triplet mining (формируем триплеты каждый батч)\n",
    "\n",
    "Стратегии обучения для online mining:\n",
    "* Batch all strategy (все примеры могут пойти в триплеты)\n",
    "* Batch hard strategy (в триплеты идут только самые сложные примеры)\n",
    "\n",
    "<img src='https://omoindrot.github.io/assets/triplet_loss/triplets.png' width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наивная реализация триплет лосса:\n",
    "\n",
    "```python\n",
    "anchor_output = ...    # shape [None, 128]\n",
    "positive_output = ...  # shape [None, 128]\n",
    "negative_output = ...  # shape [None, 128]\n",
    "\n",
    "d_pos = tf.reduce_sum(tf.square(anchor_output - positive_output), 1)\n",
    "d_neg = tf.reduce_sum(tf.square(anchor_output - negative_output), 1)\n",
    "\n",
    "loss = tf.maximum(0.0, margin + d_pos - d_neg)\n",
    "loss = tf.reduce_mean(loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Хорошо, а как предсказывать? С помощью kNN!\n",
    "\n",
    "<img src='https://pbs.twimg.com/media/DmVRIqrXcAAOvtH.jpg' width=\"400\">\n",
    "\n",
    "Плюсы kNN:\n",
    "1. Существуют [множество](https://github.com/erikbern/ann-benchmarks) методов, которые позволяют искать очень быстро.\n",
    "2. Качество на train set'е близко к 100%\n",
    "3. Вы можете легко включать и выключать любые классы из предсказаний"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как реализовать на TensorFlow?\n",
    "\n",
    "В contrib ветке TensorFlow лежит реализованный [triplet loss](https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/contrib/losses/metric_learning/triplet_semihard_loss). Попробуйте применить его на своей задаче!\n",
    "\n",
    "Или посмотрите на [эту](https://github.com/omoindrot/tensorflow-triplet-loss) реализацию triplet loss'a с нуля"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры задач\n",
    "\n",
    "* Text classification\n",
    "* Face identification\n",
    "* Chitchat\n",
    "* Image captioning\n",
    "* Твоя идея?\n",
    "\n",
    "<img src='https://omoindrot.github.io/assets/triplet_loss/triplet_loss.png' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papers/blog posts\n",
    "\n",
    "* [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/abs/1503.03832)\n",
    "* [Siamese Neural Networks for One-shot Image Recognition](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "* [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/abs/1703.07737)\n",
    "* [OpenFace 0.2.0: Higher accuracy and halved execution time](http://bamos.github.io/2016/01/19/openface-0.2.0/)\n",
    "* [Triplet Loss and Online Triplet Mining in TensorFlow](https://omoindrot.github.io/triplet-loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопросы:\n",
    "\n",
    "* В чем минусы триплет лосса? как исправить?\n",
    "* В чем преимущество triplet loss'а на задаче классификации по сравнению с log-loss'ом, если классов миллиард?\n",
    "* Сколько всего комбинаций триплетов в батче?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
