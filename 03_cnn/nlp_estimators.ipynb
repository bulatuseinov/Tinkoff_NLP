{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6iPSmuDbXvq"
   },
   "source": [
    "# CNN для классификации текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN история"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cats.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хайп вокруг CNN начался с [Alexnet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "<img src=\"img/alexnet.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/2d_convolution.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "<img src=\"img/2d_activation.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "* размер свертки\n",
    "* stride \n",
    "* padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/maxpool.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "* max pooling\n",
    "* average pooling\n",
    "* k-max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN для текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/1d_convolution.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### char cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/1d_char.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часто комбинируют свертки разного размера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/1d_multy.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Хитрости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/residual.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация текста при помощи TensorFlow Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eB6v9qVLT7q4"
   },
   "source": [
    "В этом ноубуке мы разберемся как использовать custom tensorflow estimators для классификации текста. \n",
    "Мы будем использовать tf.Data, tf.Estimator, tf.layers, word2vec эмбеддинги.\n",
    "\n",
    "\n",
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HLcya95hbyAk"
   },
   "source": [
    "### The IMDB Dataset\n",
    "\n",
    "<img src=\"img/imdb.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Мы будем использовать датасет IMDB [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), \n",
    "который состоит из $25,000$ отзывов на популярные фильмы в обучающей выборке, и $25,000$ в тестовой выборке. Мы обучим модель для бинарной классификации, которая предсказывает является ли отзыв о фильме позитивным или негативным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Заимпортим все необходимые бибилиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1523273466121,
     "user": {
      "displayName": "Julian Eisenschlos",
      "photoUrl": "//lh3.googleusercontent.com/-64ZxueD_a5k/AAAAAAAAAAI/AAAAAAAAY_I/SjD0k9jJ8Ug/s50-c-k-no/photo.jpg",
      "userId": "112692138304087361987"
     },
     "user_tz": 180
    },
    "id": "hwq4V3xwMbYk",
    "outputId": "12e9efc9-76dd-457a-de65-21b973fa8944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras предоставляет возможность легко загружать некоторые датасеты (https://keras.io/datasets/) для удобства тестирования и бенчмаркинга архитектур"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6981,
     "status": "ok",
     "timestamp": 1523273476760,
     "user": {
      "displayName": "Julian Eisenschlos",
      "photoUrl": "//lh3.googleusercontent.com/-64ZxueD_a5k/AAAAAAAAAAI/AAAAAAAAY_I/SjD0k9jJ8Ug/s50-c-k-no/photo.jpg",
      "userId": "112692138304087361987"
     },
     "user_tz": 180
    },
    "id": "RStSE6MjAR5s",
    "outputId": "0f50c910-8cc9-4c7b-9a42-cd06daf89df6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 200)\n",
      "x_test shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "sentence_size = 200\n",
    "embedding_size = 50\n",
    "model_dir = tempfile.mkdtemp()\n",
    "\n",
    "# первые индексы в словаре зарезервированы за специальными токенами \n",
    "# для паддинга, начала предложения, и для слов которые не вошли в словарь\n",
    "pad_id = 0\n",
    "start_id = 1\n",
    "oov_id = 2\n",
    "index_offset = 2\n",
    "\n",
    "print(\"Loading data...\")\n",
    "(x_train_variable, y_train), (x_test_variable, y_test) = imdb.load_data(\n",
    "    num_words=vocab_size, start_char=start_id, oov_char=oov_id,\n",
    "    index_from=index_offset)\n",
    "print(len(y_train), \"train sequences\")\n",
    "print(len(y_test), \"test sequences\")\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_train = sequence.pad_sequences(x_train_variable, \n",
    "                                 maxlen=sentence_size,\n",
    "                                 truncating='post',\n",
    "                                 padding='post',\n",
    "                                 value=pad_id)\n",
    "x_test = sequence.pad_sequences(x_test_variable, \n",
    "                                maxlen=sentence_size,\n",
    "                                truncating='post',\n",
    "                                padding='post', \n",
    "                                value=pad_id)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1BBYudtvq9w"
   },
   "source": [
    "По индексам слов можно получить их исходное представление чтобы посмотреть что было закодировано\n",
    "(например в первом примере из  train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2185,
     "status": "ok",
     "timestamp": 1523274024651,
     "user": {
      "displayName": "Julian Eisenschlos",
      "photoUrl": "//lh3.googleusercontent.com/-64ZxueD_a5k/AAAAAAAAAAI/AAAAAAAAY_I/SjD0k9jJ8Ug/s50-c-k-no/photo.jpg",
      "userId": "112692138304087361987"
     },
     "user_tz": 180
    },
    "id": "N08jsWy3vq9x",
    "outputId": "dfd55113-1190-4ac0-8a41-8141286e0360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <PAD>\n",
      "1 <START>\n",
      "2 <OOV>\n",
      "3 the\n",
      "4 and\n",
      "5 a\n",
      "6 of\n",
      "7 to\n",
      "8 is\n",
      "9 br\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <OOV> is an amazing actor and now the same being director <OOV> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <OOV> and would recommend it to everyone to watch and the fly <OOV> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <OOV> to the two little <OOV> that played the <OOV> of norman and paul they were just brilliant children are often left out of the <OOV> list i think because the stars that play them all grown up are such a big <OOV> for the whole film but these children are amazing and should be <OOV> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <OOV> with us all\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "word_inverted_index = {v + index_offset: k for k, v in word_index.items()}\n",
    "\n",
    "# Первые индексы в словаре зарезервированы для специальных токенов\n",
    "word_inverted_index[pad_id] = '<PAD>'\n",
    "word_inverted_index[start_id] = '<START>'\n",
    "word_inverted_index[oov_id] = '<OOV>'\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(i, word_inverted_index[i])\n",
    "\n",
    "def index_to_text(indexes):\n",
    "    return ' '.join([word_inverted_index[i] for i in indexes])\n",
    "\n",
    "print(index_to_text(x_train_variable[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   13,   21, ...,  225,   64,   15],\n",
       "       [   1,  193, 1152, ...,    0,    0,    0],\n",
       "       [   1,   13,   46, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,   10,    5, ...,    0,    0,    0],\n",
       "       [   1, 1445,    2, ...,    0,    0,    0],\n",
       "       [   1,   16,    5, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKhoMPMtTbul"
   },
   "source": [
    "## Tensorflow Estimator\n",
    "\n",
    "В следующей секции мы разберем как создавать кастомные эстиматоры, как их обучать и делать предсказания\n",
    "\n",
    "Детально о создании эстиматоров можно почитать [тут](https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пайплайн tf.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EFf3Z9Mbvq91"
   },
   "source": [
    "### From arrays to tensors\n",
    "\n",
    "Чтобы иметь возможность подавать данные в Tensorflow для обучения, данные нужно сконвертировать из numpy массивов в Tensors. Мы будем использовать для этого модуль `tf.data.Dataset`.\n",
    "\n",
    "Мы используем функцию `from_tensor_slices` для создания объекта `Dataset`, к которому потом можно применять различные преобразования: перемешивание данных, упаковка в батчи, повторение процесса заданное количество итераций. \n",
    "\n",
    "Пайплайн `tf.data` содержит множество других функций для закрузки данных в модель, в том числе из данных сохраненных на диске. При этом весь менеджмент памяти и другие детали ложатся на tensorflow.\n",
    "\n",
    "Для того, чтобы подавать данные в модель, нужно определить две функции: train_input_fn и eval_input_fn.\n",
    "Одна используется для подачи данных для обучения модели, а вторая для подачи тестовых данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VPLRuDRnvq92"
   },
   "outputs": [],
   "source": [
    "x_len_train = np.array([min(len(x), sentence_size) for x in x_train_variable])\n",
    "x_len_test = np.array([min(len(x), sentence_size) for x in x_test_variable])\n",
    "\n",
    "def parser(x, length, y):\n",
    "    features = {\"x\": x, \"len\": length}\n",
    "    return features, y\n",
    "\n",
    "def train_input_fn():\n",
    "    # Подача данных для обучения\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=len(x_train_variable))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    # Подача тестовых данных\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_test, x_len_test, y_test))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([200, 189, 141, ..., 184, 150, 153])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_len_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   13,   21, ...,  225,   64,   15],\n",
       "       [   1,  193, 1152, ...,    0,    0,    0],\n",
       "       [   1,   13,   46, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,   10,    5, ...,    0,    0,    0],\n",
       "       [   1, 1445,    2, ...,    0,    0,    0],\n",
       "       [   1,   16,    5, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwCuemrKvq94"
   },
   "source": [
    "### Бейзлайн\n",
    "\n",
    "Попробуем решить задачу с использованием максимально простой линейной моделью для классификации текстов.\n",
    "Линейная модель неучитывает порядок слов в предложении, поэтому подобные подходы называют Bag-of-Words (BOW, мешок слов).\n",
    "\n",
    "В качестве фичей, подаваемых в модель, мы будем использовать `tf.feature_column`. Тут [хороший туториал](https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html). В качестве модели будем использовать стандартный в TensorFlow `LinearClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 522,
     "status": "ok",
     "timestamp": 1523273591870,
     "user": {
      "displayName": "Julian Eisenschlos",
      "photoUrl": "//lh3.googleusercontent.com/-64ZxueD_a5k/AAAAAAAAAAI/AAAAAAAAY_I/SjD0k9jJ8Ug/s50-c-k-no/photo.jpg",
      "userId": "112692138304087361987"
     },
     "user_tz": 180
    },
    "id": "neNiEK31Od96",
    "outputId": "76fe017b-3c1c-4d0d-8973-e797de00118b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpugpr84q0/bow_sparse', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12fcdb390>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "column = tf.feature_column.categorical_column_with_identity('x', vocab_size)\n",
    "classifier = tf.estimator.LinearClassifier(feature_columns=[column], model_dir=os.path.join(model_dir, 'bow_sparse'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция для скоринга модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NMHErNXuBFHr"
   },
   "outputs": [],
   "source": [
    "all_classifiers = {}\n",
    "def train_and_evaluate(classifier):\n",
    "    # Save a reference to the classifier to run predictions later\n",
    "    all_classifiers[classifier.model_dir] = classifier\n",
    "    classifier.train(input_fn=train_input_fn, steps=2500)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    predictions = np.array([p['logistic'][0] for p in classifier.predict(input_fn=eval_input_fn)])\n",
    "        \n",
    "    # Reset the graph to be able to reuse name scopes\n",
    "    tf.reset_default_graph() \n",
    "    # Add a PR summary in addition to the summaries that the classifier writes\n",
    "    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, labels=y_test.astype(bool), num_thresholds=21)\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)\n",
    "        writer.add_summary(sess.run(pr), global_step=0)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 8840
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 268303,
     "status": "ok",
     "timestamp": 1523274992515,
     "user": {
      "displayName": "Julian Eisenschlos",
      "photoUrl": "//lh3.googleusercontent.com/-64ZxueD_a5k/AAAAAAAAAAI/AAAAAAAAY_I/SjD0k9jJ8Ug/s50-c-k-no/photo.jpg",
      "userId": "112692138304087361987"
     },
     "user_tz": 180
    },
    "id": "B2O7WOpCpifk",
    "outputId": "9e2e2688-4cb9-4b0b-e427-b1bc85b780de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpugpr84q0/bow_sparse/model.ckpt.\n",
      "INFO:tensorflow:loss = 69.31472, step = 1\n",
      "INFO:tensorflow:global_step/sec: 105.687\n",
      "INFO:tensorflow:loss = 39.33538, step = 101 (0.947 sec)\n",
      "INFO:tensorflow:global_step/sec: 365.823\n",
      "INFO:tensorflow:loss = 48.65559, step = 201 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.77\n",
      "INFO:tensorflow:loss = 26.427368, step = 301 (0.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 362.13\n",
      "INFO:tensorflow:loss = 31.600616, step = 401 (0.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 308.063\n",
      "INFO:tensorflow:loss = 24.079906, step = 501 (0.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 363.781\n",
      "INFO:tensorflow:loss = 27.970974, step = 601 (0.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 373.579\n",
      "INFO:tensorflow:loss = 26.432175, step = 701 (0.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.436\n",
      "INFO:tensorflow:loss = 16.015873, step = 801 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 362.381\n",
      "INFO:tensorflow:loss = 19.816557, step = 901 (0.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 306.879\n",
      "INFO:tensorflow:loss = 10.424765, step = 1001 (0.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 372.683\n",
      "INFO:tensorflow:loss = 25.995422, step = 1101 (0.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 350.276\n",
      "INFO:tensorflow:loss = 27.10348, step = 1201 (0.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.337\n",
      "INFO:tensorflow:loss = 35.47357, step = 1301 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 368.361\n",
      "INFO:tensorflow:loss = 15.502126, step = 1401 (0.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 309.561\n",
      "INFO:tensorflow:loss = 16.991268, step = 1501 (0.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 361.141\n",
      "INFO:tensorflow:loss = 24.932934, step = 1601 (0.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 369.682\n",
      "INFO:tensorflow:loss = 15.660256, step = 1701 (0.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 309.477\n",
      "INFO:tensorflow:loss = 18.313946, step = 1801 (0.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 363.528\n",
      "INFO:tensorflow:loss = 15.965464, step = 1901 (0.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.294\n",
      "INFO:tensorflow:loss = 17.865019, step = 2001 (0.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 367.598\n",
      "INFO:tensorflow:loss = 13.969057, step = 2101 (0.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 370.584\n",
      "INFO:tensorflow:loss = 21.008, step = 2201 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.685\n",
      "INFO:tensorflow:loss = 16.17179, step = 2301 (0.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 365.203\n",
      "INFO:tensorflow:loss = 13.707541, step = 2401 (0.274 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpugpr84q0/bow_sparse/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 19.926655.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-17-19:24:31\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpugpr84q0/bow_sparse/model.ckpt-2500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-17-19:24:33\n",
      "INFO:tensorflow:Saving dict for global step 2500: accuracy = 0.83568, accuracy_baseline = 0.5, auc = 0.91175616, auc_precision_recall = 0.9085719, average_loss = 0.45302027, global_step = 2500, label/mean = 0.5, loss = 45.30203, precision = 0.827199, prediction/mean = 0.51139086, recall = 0.84864\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2500: /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpugpr84q0/bow_sparse/model.ckpt-2500\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpugpr84q0/bow_sparse/model.ckpt-2500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_qAHrTDbvq99"
   },
   "source": [
    "Визуализируем, какие слова вносят наибольший вклад в результат предсказания модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1098,
     "status": "ok",
     "timestamp": 1523274052541,
     "user": {
      "displayName": "Julian Eisenschlos",
      "photoUrl": "//lh3.googleusercontent.com/-64ZxueD_a5k/AAAAAAAAAAI/AAAAAAAAY_I/SjD0k9jJ8Ug/s50-c-k-no/photo.jpg",
      "userId": "112692138304087361987"
     },
     "user_tz": 180
    },
    "id": "R7iy_8MAvq9-",
    "outputId": "2eaeb25d-7f32-4ceb-c676-c6ef8093324a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = classifier.get_variable_value('linear/linear_model/x/weights').flatten()\n",
    "sorted_indexes = np.argsort(weights)\n",
    "extremes = np.concatenate((sorted_indexes[-8:], sorted_indexes[:8]))\n",
    "extreme_weights = sorted([(weights[i], word_inverted_index[i]) for i in extremes])\n",
    "\n",
    "y_pos = np.arange(len(extreme_weights))\n",
    "plt.bar(y_pos, [pair[0] for pair in extreme_weights], align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, [pair[1] for pair in extreme_weights], rotation=45, ha='right')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Most significant tokens') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptorLYCwArTp"
   },
   "source": [
    "### Embeddings\n",
    "\n",
    "Теперь добавим в наше решение эмбеддинги слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 8979
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 274050,
     "status": "ok",
     "timestamp": 1520355496237,
     "user": {
      "displayName": "Sebastian Ruder",
      "photoUrl": "//lh5.googleusercontent.com/-_RsKB476gus/AAAAAAAAAAI/AAAAAAAAVys/bDJNFfUAEUU/s50-c-k-no/photo.jpg",
      "userId": "101962136952284559776"
     },
     "user_tz": 0
    },
    "id": "IpJrW8_zvq-D",
    "outputId": "2d3d6041-1ac3-468e-cd8c-9760c558f30e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpwemhzmwh/bow_embeddings', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13d7d1080>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpwemhzmwh/bow_embeddings/model.ckpt.\n",
      "INFO:tensorflow:loss = 69.25685, step = 1\n",
      "INFO:tensorflow:global_step/sec: 79.0433\n",
      "INFO:tensorflow:loss = 65.2278, step = 101 (1.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.215\n",
      "INFO:tensorflow:loss = 42.035618, step = 201 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.343\n",
      "INFO:tensorflow:loss = 31.61026, step = 301 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.27\n",
      "INFO:tensorflow:loss = 31.255138, step = 401 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.129\n",
      "INFO:tensorflow:loss = 24.817356, step = 501 (0.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.06\n",
      "INFO:tensorflow:loss = 29.815342, step = 601 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.637\n",
      "INFO:tensorflow:loss = 37.979954, step = 701 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.055\n",
      "INFO:tensorflow:loss = 30.614283, step = 801 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.037\n",
      "INFO:tensorflow:loss = 32.077045, step = 901 (0.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.04\n",
      "INFO:tensorflow:loss = 19.549263, step = 1001 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.271\n",
      "INFO:tensorflow:loss = 23.889355, step = 1101 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.145\n",
      "INFO:tensorflow:loss = 25.83942, step = 1201 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.989\n",
      "INFO:tensorflow:loss = 30.823725, step = 1301 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.635\n",
      "INFO:tensorflow:loss = 21.189545, step = 1401 (0.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.518\n",
      "INFO:tensorflow:loss = 25.582863, step = 1501 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.37\n",
      "INFO:tensorflow:loss = 19.854801, step = 1601 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.316\n",
      "INFO:tensorflow:loss = 16.395977, step = 1701 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.436\n",
      "INFO:tensorflow:loss = 19.462187, step = 1801 (0.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.706\n",
      "INFO:tensorflow:loss = 14.669487, step = 1901 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.406\n",
      "INFO:tensorflow:loss = 9.368029, step = 2001 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.653\n",
      "INFO:tensorflow:loss = 22.176805, step = 2101 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.991\n",
      "INFO:tensorflow:loss = 19.869043, step = 2201 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.146\n",
      "INFO:tensorflow:loss = 23.995567, step = 2301 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.728\n",
      "INFO:tensorflow:loss = 13.014556, step = 2401 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.173\n",
      "INFO:tensorflow:loss = 17.351915, step = 2501 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.364\n",
      "INFO:tensorflow:loss = 24.675585, step = 2601 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.154\n",
      "INFO:tensorflow:loss = 23.943007, step = 2701 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.772\n",
      "INFO:tensorflow:loss = 17.08852, step = 2801 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.982\n",
      "INFO:tensorflow:loss = 19.575562, step = 2901 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.734\n",
      "INFO:tensorflow:loss = 23.166739, step = 3001 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.597\n",
      "INFO:tensorflow:loss = 11.670891, step = 3101 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.364\n",
      "INFO:tensorflow:loss = 15.49542, step = 3201 (0.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.161\n",
      "INFO:tensorflow:loss = 20.156843, step = 3301 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.157\n",
      "INFO:tensorflow:loss = 19.99774, step = 3401 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.792\n",
      "INFO:tensorflow:loss = 23.388285, step = 3501 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.201\n",
      "INFO:tensorflow:loss = 17.748308, step = 3601 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.247\n",
      "INFO:tensorflow:loss = 25.671978, step = 3701 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.906\n",
      "INFO:tensorflow:loss = 10.959495, step = 3801 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.3\n",
      "INFO:tensorflow:loss = 20.000854, step = 3901 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.075\n",
      "INFO:tensorflow:loss = 20.60051, step = 4001 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.147\n",
      "INFO:tensorflow:loss = 18.712547, step = 4101 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.949\n",
      "INFO:tensorflow:loss = 13.053994, step = 4201 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.185\n",
      "INFO:tensorflow:loss = 7.901802, step = 4301 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.339\n",
      "INFO:tensorflow:loss = 20.20562, step = 4401 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.187\n",
      "INFO:tensorflow:loss = 9.306433, step = 4501 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.232\n",
      "INFO:tensorflow:loss = 17.173588, step = 4601 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.059\n",
      "INFO:tensorflow:loss = 11.8849745, step = 4701 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.297\n",
      "INFO:tensorflow:loss = 11.907893, step = 4801 (0.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.778\n",
      "INFO:tensorflow:loss = 11.397637, step = 4901 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.588\n",
      "INFO:tensorflow:loss = 12.447871, step = 5001 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.606\n",
      "INFO:tensorflow:loss = 14.454758, step = 5101 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.643\n",
      "INFO:tensorflow:loss = 9.046384, step = 5201 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.767\n",
      "INFO:tensorflow:loss = 9.704604, step = 5301 (0.443 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.387\n",
      "INFO:tensorflow:loss = 15.807195, step = 5401 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.473\n",
      "INFO:tensorflow:loss = 15.679273, step = 5501 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.188\n",
      "INFO:tensorflow:loss = 14.451113, step = 5601 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.053\n",
      "INFO:tensorflow:loss = 13.413496, step = 5701 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.397\n",
      "INFO:tensorflow:loss = 15.280346, step = 5801 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.524\n",
      "INFO:tensorflow:loss = 12.157942, step = 5901 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.922\n",
      "INFO:tensorflow:loss = 8.912283, step = 6001 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.468\n",
      "INFO:tensorflow:loss = 7.714976, step = 6101 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.517\n",
      "INFO:tensorflow:loss = 12.297983, step = 6201 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.056\n",
      "INFO:tensorflow:loss = 7.1935887, step = 6301 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.593\n",
      "INFO:tensorflow:loss = 11.048397, step = 6401 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.33\n",
      "INFO:tensorflow:loss = 4.9167104, step = 6501 (0.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.841\n",
      "INFO:tensorflow:loss = 6.1029067, step = 6601 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.599\n",
      "INFO:tensorflow:loss = 12.173414, step = 6701 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.944\n",
      "INFO:tensorflow:loss = 7.131657, step = 6801 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.167\n",
      "INFO:tensorflow:loss = 12.957524, step = 6901 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.028\n",
      "INFO:tensorflow:loss = 14.610855, step = 7001 (0.453 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 253.69\n",
      "INFO:tensorflow:loss = 10.179875, step = 7101 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.484\n",
      "INFO:tensorflow:loss = 12.793192, step = 7201 (0.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.009\n",
      "INFO:tensorflow:loss = 4.067693, step = 7301 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.906\n",
      "INFO:tensorflow:loss = 8.528903, step = 7401 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.871\n",
      "INFO:tensorflow:loss = 10.159124, step = 7501 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.425\n",
      "INFO:tensorflow:loss = 3.5287805, step = 7601 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.92\n",
      "INFO:tensorflow:loss = 7.590412, step = 7701 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.605\n",
      "INFO:tensorflow:loss = 3.8217783, step = 7801 (0.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.094\n",
      "INFO:tensorflow:loss = 8.876156, step = 7901 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.262\n",
      "INFO:tensorflow:loss = 2.727556, step = 8001 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.328\n",
      "INFO:tensorflow:loss = 7.336302, step = 8101 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.758\n",
      "INFO:tensorflow:loss = 8.385338, step = 8201 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.099\n",
      "INFO:tensorflow:loss = 5.7349553, step = 8301 (0.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.04\n",
      "INFO:tensorflow:loss = 5.884555, step = 8401 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.709\n",
      "INFO:tensorflow:loss = 6.673216, step = 8501 (0.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.065\n",
      "INFO:tensorflow:loss = 8.22224, step = 8601 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.703\n",
      "INFO:tensorflow:loss = 4.651144, step = 8701 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.065\n",
      "INFO:tensorflow:loss = 4.633155, step = 8801 (0.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.488\n",
      "INFO:tensorflow:loss = 4.4410024, step = 8901 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.039\n",
      "INFO:tensorflow:loss = 5.173585, step = 9001 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.539\n",
      "INFO:tensorflow:loss = 3.3224223, step = 9101 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.239\n",
      "INFO:tensorflow:loss = 4.9639955, step = 9201 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.279\n",
      "INFO:tensorflow:loss = 4.079935, step = 9301 (0.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.855\n",
      "INFO:tensorflow:loss = 3.2325392, step = 9401 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.406\n",
      "INFO:tensorflow:loss = 3.9880464, step = 9501 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.005\n",
      "INFO:tensorflow:loss = 5.1513214, step = 9601 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.045\n",
      "INFO:tensorflow:loss = 3.0797906, step = 9701 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.252\n",
      "INFO:tensorflow:loss = 2.8233426, step = 9801 (0.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.811\n",
      "INFO:tensorflow:loss = 2.79433, step = 9901 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.086\n",
      "INFO:tensorflow:loss = 2.6089497, step = 10001 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.62\n",
      "INFO:tensorflow:loss = 5.327946, step = 10101 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.877\n",
      "INFO:tensorflow:loss = 2.2384496, step = 10201 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.233\n",
      "INFO:tensorflow:loss = 2.7059865, step = 10301 (0.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.259\n",
      "INFO:tensorflow:loss = 2.2448413, step = 10401 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.064\n",
      "INFO:tensorflow:loss = 2.5870266, step = 10501 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.586\n",
      "INFO:tensorflow:loss = 3.1702523, step = 10601 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.213\n",
      "INFO:tensorflow:loss = 4.7310605, step = 10701 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.517\n",
      "INFO:tensorflow:loss = 2.2202086, step = 10801 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.244\n",
      "INFO:tensorflow:loss = 2.1257162, step = 10901 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.33\n",
      "INFO:tensorflow:loss = 1.75995, step = 11001 (0.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.566\n",
      "INFO:tensorflow:loss = 2.4212458, step = 11101 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.293\n",
      "INFO:tensorflow:loss = 1.4503361, step = 11201 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.913\n",
      "INFO:tensorflow:loss = 3.2229042, step = 11301 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.494\n",
      "INFO:tensorflow:loss = 1.5774121, step = 11401 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.152\n",
      "INFO:tensorflow:loss = 1.8048997, step = 11501 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.086\n",
      "INFO:tensorflow:loss = 1.251031, step = 11601 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.253\n",
      "INFO:tensorflow:loss = 1.7765968, step = 11701 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.545\n",
      "INFO:tensorflow:loss = 2.117029, step = 11801 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.484\n",
      "INFO:tensorflow:loss = 0.94529665, step = 11901 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.505\n",
      "INFO:tensorflow:loss = 1.43847, step = 12001 (0.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.253\n",
      "INFO:tensorflow:loss = 2.8797352, step = 12101 (0.421 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.503\n",
      "INFO:tensorflow:loss = 1.165749, step = 12201 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.728\n",
      "INFO:tensorflow:loss = 3.098255, step = 12301 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.199\n",
      "INFO:tensorflow:loss = 1.5535349, step = 12401 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.585\n",
      "INFO:tensorflow:loss = 1.3602763, step = 12501 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.457\n",
      "INFO:tensorflow:loss = 2.0151134, step = 12601 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.942\n",
      "INFO:tensorflow:loss = 1.4955431, step = 12701 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.48\n",
      "INFO:tensorflow:loss = 1.9129106, step = 12801 (0.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.682\n",
      "INFO:tensorflow:loss = 1.2426305, step = 12901 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.84\n",
      "INFO:tensorflow:loss = 0.80871266, step = 13001 (0.443 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.885\n",
      "INFO:tensorflow:loss = 2.262364, step = 13101 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.735\n",
      "INFO:tensorflow:loss = 2.4025416, step = 13201 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.16\n",
      "INFO:tensorflow:loss = 0.7852245, step = 13301 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.433\n",
      "INFO:tensorflow:loss = 0.8543567, step = 13401 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.82\n",
      "INFO:tensorflow:loss = 1.010288, step = 13501 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.657\n",
      "INFO:tensorflow:loss = 1.0758277, step = 13601 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.902\n",
      "INFO:tensorflow:loss = 2.2028093, step = 13701 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.71\n",
      "INFO:tensorflow:loss = 1.731795, step = 13801 (0.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.396\n",
      "INFO:tensorflow:loss = 2.947014, step = 13901 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.332\n",
      "INFO:tensorflow:loss = 0.8136141, step = 14001 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.93\n",
      "INFO:tensorflow:loss = 1.131875, step = 14101 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.573\n",
      "INFO:tensorflow:loss = 1.073502, step = 14201 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.335\n",
      "INFO:tensorflow:loss = 1.6582417, step = 14301 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.893\n",
      "INFO:tensorflow:loss = 1.1582754, step = 14401 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.621\n",
      "INFO:tensorflow:loss = 0.71297103, step = 14501 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.408\n",
      "INFO:tensorflow:loss = 0.8134698, step = 14601 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.584\n",
      "INFO:tensorflow:loss = 0.7941223, step = 14701 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.656\n",
      "INFO:tensorflow:loss = 1.1266643, step = 14801 (0.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.413\n",
      "INFO:tensorflow:loss = 0.94452167, step = 14901 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.897\n",
      "INFO:tensorflow:loss = 0.77961546, step = 15001 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.007\n",
      "INFO:tensorflow:loss = 0.81723565, step = 15101 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.64\n",
      "INFO:tensorflow:loss = 1.1588457, step = 15201 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.919\n",
      "INFO:tensorflow:loss = 0.47180542, step = 15301 (0.449 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 246.128\n",
      "INFO:tensorflow:loss = 1.0129527, step = 15401 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.825\n",
      "INFO:tensorflow:loss = 0.80087864, step = 15501 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.695\n",
      "INFO:tensorflow:loss = 0.9187151, step = 15601 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.645\n",
      "INFO:tensorflow:loss = 1.243428, step = 15701 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.774\n",
      "INFO:tensorflow:loss = 1.1233305, step = 15801 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.724\n",
      "INFO:tensorflow:loss = 0.84714967, step = 15901 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.118\n",
      "INFO:tensorflow:loss = 0.8207511, step = 16001 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.222\n",
      "INFO:tensorflow:loss = 0.39254928, step = 16101 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.971\n",
      "INFO:tensorflow:loss = 0.9005867, step = 16201 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.344\n",
      "INFO:tensorflow:loss = 0.6766658, step = 16301 (0.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.726\n",
      "INFO:tensorflow:loss = 0.843516, step = 16401 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.67\n",
      "INFO:tensorflow:loss = 0.61824137, step = 16501 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.367\n",
      "INFO:tensorflow:loss = 0.2995269, step = 16601 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.395\n",
      "INFO:tensorflow:loss = 1.2672868, step = 16701 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.292\n",
      "INFO:tensorflow:loss = 0.5081913, step = 16801 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.966\n",
      "INFO:tensorflow:loss = 0.6706321, step = 16901 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.111\n",
      "INFO:tensorflow:loss = 0.7554768, step = 17001 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.709\n",
      "INFO:tensorflow:loss = 0.5383704, step = 17101 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.949\n",
      "INFO:tensorflow:loss = 0.33225003, step = 17201 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.418\n",
      "INFO:tensorflow:loss = 1.8609184, step = 17301 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.502\n",
      "INFO:tensorflow:loss = 0.48237437, step = 17401 (0.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.929\n",
      "INFO:tensorflow:loss = 0.74273366, step = 17501 (0.443 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.859\n",
      "INFO:tensorflow:loss = 0.7797602, step = 17601 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.775\n",
      "INFO:tensorflow:loss = 0.45238778, step = 17701 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.894\n",
      "INFO:tensorflow:loss = 0.4592034, step = 17801 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.513\n",
      "INFO:tensorflow:loss = 0.24668017, step = 17901 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.895\n",
      "INFO:tensorflow:loss = 0.48604354, step = 18001 (0.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.873\n",
      "INFO:tensorflow:loss = 0.8271919, step = 18101 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.73\n",
      "INFO:tensorflow:loss = 0.77175903, step = 18201 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.876\n",
      "INFO:tensorflow:loss = 0.60299766, step = 18301 (0.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.602\n",
      "INFO:tensorflow:loss = 0.6508059, step = 18401 (0.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.52\n",
      "INFO:tensorflow:loss = 0.3037425, step = 18501 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.386\n",
      "INFO:tensorflow:loss = 0.49707726, step = 18601 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.426\n",
      "INFO:tensorflow:loss = 0.62591994, step = 18701 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.77\n",
      "INFO:tensorflow:loss = 0.77415526, step = 18801 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.77\n",
      "INFO:tensorflow:loss = 0.43191102, step = 18901 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.568\n",
      "INFO:tensorflow:loss = 0.48608458, step = 19001 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.643\n",
      "INFO:tensorflow:loss = 0.5913677, step = 19101 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.877\n",
      "INFO:tensorflow:loss = 0.8552129, step = 19201 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.822\n",
      "INFO:tensorflow:loss = 0.4390275, step = 19301 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.313\n",
      "INFO:tensorflow:loss = 0.5855855, step = 19401 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.538\n",
      "INFO:tensorflow:loss = 0.3966334, step = 19501 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.008\n",
      "INFO:tensorflow:loss = 0.28638506, step = 19601 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.13\n",
      "INFO:tensorflow:loss = 0.53499913, step = 19701 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.497\n",
      "INFO:tensorflow:loss = 0.48751527, step = 19801 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.898\n",
      "INFO:tensorflow:loss = 0.7340394, step = 19901 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.411\n",
      "INFO:tensorflow:loss = 0.4746681, step = 20001 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.67\n",
      "INFO:tensorflow:loss = 0.16222188, step = 20101 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.177\n",
      "INFO:tensorflow:loss = 0.25109252, step = 20201 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.561\n",
      "INFO:tensorflow:loss = 0.32832652, step = 20301 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.164\n",
      "INFO:tensorflow:loss = 0.3512829, step = 20401 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.539\n",
      "INFO:tensorflow:loss = 0.2733004, step = 20501 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.189\n",
      "INFO:tensorflow:loss = 0.35376808, step = 20601 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.773\n",
      "INFO:tensorflow:loss = 0.21492779, step = 20701 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.93\n",
      "INFO:tensorflow:loss = 0.19055504, step = 20801 (0.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.955\n",
      "INFO:tensorflow:loss = 0.3785394, step = 20901 (0.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.12\n",
      "INFO:tensorflow:loss = 0.40226048, step = 21001 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.645\n",
      "INFO:tensorflow:loss = 0.36473584, step = 21101 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.454\n",
      "INFO:tensorflow:loss = 0.2766073, step = 21201 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.958\n",
      "INFO:tensorflow:loss = 0.3551911, step = 21301 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.018\n",
      "INFO:tensorflow:loss = 0.60863036, step = 21401 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.686\n",
      "INFO:tensorflow:loss = 0.3716129, step = 21501 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.964\n",
      "INFO:tensorflow:loss = 0.2668708, step = 21601 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.845\n",
      "INFO:tensorflow:loss = 0.5625186, step = 21701 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.799\n",
      "INFO:tensorflow:loss = 0.1823523, step = 21801 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.068\n",
      "INFO:tensorflow:loss = 0.16947012, step = 21901 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.375\n",
      "INFO:tensorflow:loss = 0.34062567, step = 22001 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.374\n",
      "INFO:tensorflow:loss = 0.41059723, step = 22101 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.216\n",
      "INFO:tensorflow:loss = 0.40361324, step = 22201 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.567\n",
      "INFO:tensorflow:loss = 0.35614166, step = 22301 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.355\n",
      "INFO:tensorflow:loss = 0.39627934, step = 22401 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.972\n",
      "INFO:tensorflow:loss = 0.18067338, step = 22501 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.137\n",
      "INFO:tensorflow:loss = 0.8535278, step = 22601 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.577\n",
      "INFO:tensorflow:loss = 0.24448666, step = 22701 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.727\n",
      "INFO:tensorflow:loss = 0.22612835, step = 22801 (0.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.279\n",
      "INFO:tensorflow:loss = 0.3170383, step = 22901 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.372\n",
      "INFO:tensorflow:loss = 0.5392923, step = 23001 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.121\n",
      "INFO:tensorflow:loss = 0.33757398, step = 23101 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.267\n",
      "INFO:tensorflow:loss = 0.14497516, step = 23201 (0.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.56\n",
      "INFO:tensorflow:loss = 0.10990333, step = 23301 (0.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.704\n",
      "INFO:tensorflow:loss = 0.1582117, step = 23401 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.358\n",
      "INFO:tensorflow:loss = 0.23549801, step = 23501 (0.475 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 259.257\n",
      "INFO:tensorflow:loss = 0.2739224, step = 23601 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.002\n",
      "INFO:tensorflow:loss = 0.18472819, step = 23701 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.074\n",
      "INFO:tensorflow:loss = 0.31682017, step = 23801 (0.796 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.995\n",
      "INFO:tensorflow:loss = 0.10568954, step = 23901 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.468\n",
      "INFO:tensorflow:loss = 0.22410764, step = 24001 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.895\n",
      "INFO:tensorflow:loss = 0.16282889, step = 24101 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.866\n",
      "INFO:tensorflow:loss = 0.18643825, step = 24201 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.074\n",
      "INFO:tensorflow:loss = 0.22405721, step = 24301 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.529\n",
      "INFO:tensorflow:loss = 0.3045269, step = 24401 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.263\n",
      "INFO:tensorflow:loss = 0.18142346, step = 24501 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.953\n",
      "INFO:tensorflow:loss = 0.33394533, step = 24601 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.818\n",
      "INFO:tensorflow:loss = 0.1629366, step = 24701 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.469\n",
      "INFO:tensorflow:loss = 0.29431745, step = 24801 (0.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.737\n",
      "INFO:tensorflow:loss = 0.23052469, step = 24901 (0.426 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 25000 into /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpwemhzmwh/bow_embeddings/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.25197968.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-13-19:18:57\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpwemhzmwh/bow_embeddings/model.ckpt-25000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-13-19:19:00\n",
      "INFO:tensorflow:Saving dict for global step 25000: accuracy = 0.82164, accuracy_baseline = 0.5, auc = 0.86391234, auc_precision_recall = 0.88465136, average_loss = 1.6344715, global_step = 25000, label/mean = 0.5, loss = 163.44716, precision = 0.8297925, prediction/mean = 0.48746267, recall = 0.80928\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 25000: /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpwemhzmwh/bow_embeddings/model.ckpt-25000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpwemhzmwh/bow_embeddings/model.ckpt-25000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "word_embedding_column = tf.feature_column.embedding_column(column, dimension=embedding_size)\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    hidden_units=[100],\n",
    "    feature_columns=[word_embedding_column], \n",
    "    model_dir=os.path.join(model_dir, 'bow_embeddings'))\n",
    "train_and_evaluate(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9q7qfXrvq-J"
   },
   "source": [
    "Визуализируем эмбеддинги при помощи [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n",
    "\n",
    "В TensorFlow для этого есть [standalone projector visualizer](http://projector.tensorflow.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ujVD4Yv0vq-J"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir, 'metadata.tsv'), 'w', encoding=\"utf-8\") as f:\n",
    "    f.write('label\\n')\n",
    "    for index in range(0, vocab_size):\n",
    "        f.write(word_inverted_index[index] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9SKqlRpvq-N"
   },
   "source": [
    "### Convolutions\n",
    "\n",
    "Применим 1d свертки по словам, чтобы предсказывать лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdIuS0-65eNB"
   },
   "source": [
    "### Создание кастомного estimator`а\n",
    "\n",
    "Фреймворк `tf.estimator`предоставляет высокоуровневые API для обучения ML моделей,методы `train()`, `evaluate()` и `predict()`, автоматическое сохранение чекпоинтов, методы загрузки, инициализации, сервинга модели, создание сессии и графа из коробки. \n",
    "\n",
    "Одно из важных преимуществ использования именно эстиматоров состоит в том, что один и тот же код можно использовать без изменений на CPU, GPU и даже распределенной конфигурации GPU.\n",
    "\n",
    "В `tf` есть несколько простых стандартных эстиматоров. Но в большинстве случаев удобнее написть свой кастомный эстиматор, где можно использовать любую архитектуру нейронной сети. [Тут](https://www.tensorflow.org/extend/estimators) можно почитать хорошое объяснение про то как создавать кастомные эстиматоры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1314,
     "status": "ok",
     "timestamp": 1520355499529,
     "user": {
      "displayName": "Sebastian Ruder",
      "photoUrl": "//lh5.googleusercontent.com/-_RsKB476gus/AAAAAAAAAAI/AAAAAAAAVys/bDJNFfUAEUU/s50-c-k-no/photo.jpg",
      "userId": "101962136952284559776"
     },
     "user_tz": 0
    },
    "id": "Wf-sGICDejgx",
    "outputId": "3013573a-b39f-426b-80d3-0630b7107149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpugpr84q0/cnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13595b2e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "head = tf.contrib.estimator.binary_classification_head()\n",
    "\n",
    "def cnn_model_fn(features, labels, mode, params):    \n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        initializer=params['embedding_initializer'])\n",
    "    \n",
    "    training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    dropout_emb = tf.layers.dropout(inputs=input_layer, \n",
    "                                    rate=0.2, \n",
    "                                    training=training)\n",
    "\n",
    "    conv = tf.layers.conv1d(\n",
    "        inputs=dropout_emb,\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Global Max Pooling\n",
    "    pool = tf.reduce_max(input_tensor=conv, axis=1)\n",
    "    \n",
    "    hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)\n",
    "    \n",
    "    dropout_hidden = tf.layers.dropout(inputs=hidden, \n",
    "                                       rate=0.2, \n",
    "                                       training=training)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=dropout_hidden, units=1)\n",
    "    \n",
    "    # This will be None when predicting\n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "    print(labels.shape)    \n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    def _train_op_fn(loss):\n",
    "        return optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "\n",
    "    return head.create_estimator_spec(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        mode=mode,\n",
    "        logits=logits, \n",
    "        train_op_fn=_train_op_fn)\n",
    "  \n",
    "params = {'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0)}\n",
    "cnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                                        model_dir=os.path.join(model_dir, 'cnn'),\n",
    "                                        params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 8979
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 197228,
     "status": "ok",
     "timestamp": 1520355697562,
     "user": {
      "displayName": "Sebastian Ruder",
      "photoUrl": "//lh5.googleusercontent.com/-_RsKB476gus/AAAAAAAAAAI/AAAAAAAAVys/bDJNFfUAEUU/s50-c-k-no/photo.jpg",
      "userId": "101962136952284559776"
     },
     "user_tz": 0
    },
    "id": "_ndMYmQ5qNk2",
    "outputId": "8a8ec2d2-d467-4e0d-a964-3eaba1258f50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "(?, 1)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpugpr84q0/cnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.77835745, step = 1\n",
      "INFO:tensorflow:global_step/sec: 24.3344\n",
      "INFO:tensorflow:loss = 0.69506854, step = 101 (4.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.0765\n",
      "INFO:tensorflow:loss = 0.645625, step = 201 (2.772 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.254\n",
      "INFO:tensorflow:loss = 0.5851082, step = 301 (3.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.7492\n",
      "INFO:tensorflow:loss = 0.5621565, step = 401 (3.054 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.952\n",
      "INFO:tensorflow:loss = 0.5764883, step = 501 (2.781 sec)\n",
      "INFO:tensorflow:global_step/sec: 28.5055\n",
      "INFO:tensorflow:loss = 0.40669453, step = 601 (3.508 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.3486\n",
      "INFO:tensorflow:loss = 0.38280973, step = 701 (3.657 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.9081\n",
      "INFO:tensorflow:loss = 0.3724882, step = 801 (2.948 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.9917\n",
      "INFO:tensorflow:loss = 0.36398038, step = 901 (2.778 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.4275\n",
      "INFO:tensorflow:loss = 0.44343087, step = 1001 (2.823 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8054\n",
      "INFO:tensorflow:loss = 0.3750078, step = 1101 (3.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.8021\n",
      "INFO:tensorflow:loss = 0.4455945, step = 1201 (3.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.9221\n",
      "INFO:tensorflow:loss = 0.35445377, step = 1301 (3.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.2589\n",
      "INFO:tensorflow:loss = 0.287166, step = 1401 (3.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.3295\n",
      "INFO:tensorflow:loss = 0.46331462, step = 1501 (3.000 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.6606\n",
      "INFO:tensorflow:loss = 0.33280927, step = 1601 (3.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.5724\n",
      "INFO:tensorflow:loss = 0.36867657, step = 1701 (3.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.3398\n",
      "INFO:tensorflow:loss = 0.23153862, step = 1801 (3.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.1055\n",
      "INFO:tensorflow:loss = 0.2794578, step = 1901 (3.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.4683\n",
      "INFO:tensorflow:loss = 0.21457827, step = 2001 (3.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.6247\n",
      "INFO:tensorflow:loss = 0.29697686, step = 2101 (3.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.5862\n",
      "INFO:tensorflow:loss = 0.18916415, step = 2201 (3.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.594\n",
      "INFO:tensorflow:loss = 0.17467728, step = 2301 (3.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.6953\n",
      "INFO:tensorflow:loss = 0.35090148, step = 2401 (3.155 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpugpr84q0/cnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.21471822.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "(?, 1)\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-17-20:01:00\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpugpr84q0/cnn/model.ckpt-2500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-17-20:01:04\n",
      "INFO:tensorflow:Saving dict for global step 2500: accuracy = 0.84644, accuracy_baseline = 0.5, auc = 0.9259016, auc_precision_recall = 0.9232218, average_loss = 0.36183152, global_step = 2500, label/mean = 0.5, loss = 0.36183158, precision = 0.8523883, prediction/mean = 0.49190745, recall = 0.838\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2500: /var/folders/zw/2w6s_cjx4h9dj9fjz5t8tr800000gn/T/tmpugpr84q0/cnn/model.ckpt-2500\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8f97c09843e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-b5efd0651c94>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(classifier)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logistic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Reset the graph to be able to reuse name scopes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b5efd0651c94>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logistic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Reset the graph to be able to reuse name scopes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    575\u001b[0m             input_fn, model_fn_lib.ModeKeys.PREDICT)\n\u001b[1;32m    576\u001b[0m         estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 577\u001b[0;31m             features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;31m# Call to warm_start has to be after model_fn is called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c7f05ebb7cf3>\u001b[0m in \u001b[0;36mcnn_model_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(cnn_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-wDDowAvq-X"
   },
   "source": [
    "### Предобученные вектора\n",
    "\n",
    "Попробуем исользовать предобученные эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 116108,
     "status": "ok",
     "timestamp": 1520355813761,
     "user": {
      "displayName": "Sebastian Ruder",
      "photoUrl": "//lh5.googleusercontent.com/-_RsKB476gus/AAAAAAAAAAI/AAAAAAAAVys/bDJNFfUAEUU/s50-c-k-no/photo.jpg",
      "userId": "101962136952284559776"
     },
     "user_tz": 0
    },
    "id": "7zbWoklJtzRP",
    "outputId": "a03a4db2-cc6a-4a43-8f5d-c4aeac7c00be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-02-19 15:34:09--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving proxy.tcsbank.ru (proxy.tcsbank.ru)... 10.218.11.51, 10.218.6.48, 10.218.6.49, ...\n",
      "Connecting to proxy.tcsbank.ru (proxy.tcsbank.ru)|10.218.11.51|:8080... connected.\n",
      "Proxy request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2019-02-19 15:34:10--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to proxy.tcsbank.ru (proxy.tcsbank.ru)|10.218.11.51|:8080... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip         83%[===============>    ] 688.10M   669KB/s    eta 3m 40s "
     ]
    }
   ],
   "source": [
    "if not os.path.exists('glove.6B.zip'):\n",
    "    ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "if not os.path.exists('glove.6B.50d.txt'):\n",
    "    ! unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7587,
     "status": "ok",
     "timestamp": 1520355821435,
     "user": {
      "displayName": "Sebastian Ruder",
      "photoUrl": "//lh5.googleusercontent.com/-_RsKB476gus/AAAAAAAAAAI/AAAAAAAAVys/bDJNFfUAEUU/s50-c-k-no/photo.jpg",
      "userId": "101962136952284559776"
     },
     "user_tz": 0
    },
    "id": "S5m1Fkj4vq-Y",
    "outputId": "5c525191-ae6f-45aa-9047-ea0510746451"
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            w = values[0]\n",
    "            vectors = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[w] = vectors\n",
    "\n",
    "    embedding_matrix = np.random.uniform(-1, 1, size=(vocab_size, embedding_size))\n",
    "    num_loaded = 0\n",
    "    for w, i in word_index.items():\n",
    "        v = embeddings.get(w)\n",
    "        if v is not None and i < vocab_size:\n",
    "            embedding_matrix[i] = v\n",
    "            num_loaded += 1\n",
    "    print('Successfully loaded pretrained embeddings for '\n",
    "          f'{num_loaded}/{vocab_size} words.')\n",
    "    embedding_matrix = embedding_matrix.astype(np.float32)\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = load_glove_embeddings('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LwE2M5kKY44"
   },
   "source": [
    "Для этого будем подавать в эстиматор инициализатор эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 8962
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 206590,
     "status": "ok",
     "timestamp": 1520356028896,
     "user": {
      "displayName": "Sebastian Ruder",
      "photoUrl": "//lh5.googleusercontent.com/-_RsKB476gus/AAAAAAAAAAI/AAAAAAAAVys/bDJNFfUAEUU/s50-c-k-no/photo.jpg",
      "userId": "101962136952284559776"
     },
     "user_tz": 0
    },
    "id": "1uWwWkqjFbRE",
    "outputId": "466bc012-df13-428d-d91d-32aae3c51756"
   },
   "outputs": [],
   "source": [
    "def my_initializer(shape=None, dtype=tf.float32, partition_info=None):\n",
    "    assert dtype is tf.float32\n",
    "    return embedding_matrix\n",
    "\n",
    "params = {'embedding_initializer': my_initializer}\n",
    "cnn_pretrained_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                                        model_dir=os.path.join(model_dir, 'cnn_pretrained'),\n",
    "                                        params=params)\n",
    "train_and_evaluate(cnn_pretrained_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5l6_C5uTVk4"
   },
   "source": [
    "## Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKVCyX68vq-c"
   },
   "source": [
    "### Запуск TensorBoard\n",
    "\n",
    "Проанализируем, какие метрии мы получили\n",
    "\n",
    "Чтобы запустить tensorboard, нужно выполнить команду\n",
    "```bash\n",
    "> tensorboard --logdir={model_dir}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_2dw3iYOSuNg"
   },
   "source": [
    "### Предсказания модели\n",
    "\n",
    "Чтобы получить предсказания модели на новых примерах можно использовать метод `predict` у объекта `Estimator`, \n",
    "который подгружает последний сохранненный checkpoint для каждой модели и прогоняет через можель данные новых примеров. \n",
    "\n",
    "Но перед подачей данных в модель их нужно очистить, токенизировать и отобразить каждое слова в соответствущий ему индекс.\n",
    "\n",
    "[!] Наличие только файла с сохраненной моделью не является достаточным, чтобы сгенерировать предсказания, т.к. код для создания эстиматора используется для сопоставления сохраненных весов и соответствующих тензоров в графе, поэтому хорошей практикой является сохранение модели в одной папке с кодом, который используется для ее создания. \n",
    "\n",
    "[!] Создание файла с моделью, которую можно полностью восстановить из файла возможно при помощи класса [SavedModel](https://www.tensorflow.org/programmers_guide/saved_model#using_savedmodel_with_estimators), \n",
    "который обычно используется при сохранении модели для сервинга [TensorFlow Serving](https://github.com/tensorflow/serving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 937,
     "status": "ok",
     "timestamp": 1523274345221,
     "user": {
      "displayName": "Julian Eisenschlos",
      "photoUrl": "//lh3.googleusercontent.com/-64ZxueD_a5k/AAAAAAAAAAI/AAAAAAAAY_I/SjD0k9jJ8Ug/s50-c-k-no/photo.jpg",
      "userId": "112692138304087361987"
     },
     "user_tz": 180
    },
    "id": "34K8O0bTNj-b",
    "outputId": "eef27d40-63ce-4863-8720-7669a39ba03a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def text_to_index(sentence):\n",
    "    # Remove punctuation characters except for the apostrophe\n",
    "    translator = str.maketrans('', '', string.punctuation.replace(\"'\", ''))\n",
    "    tokens = sentence.translate(translator).lower().split()\n",
    "    return np.array([1] + [word_index[t] if t in word_index else oov_id for t in tokens])\n",
    "\n",
    "def print_predictions(sentences):\n",
    "    indexes = [text_to_index(sentence) for sentence in sentences]\n",
    "    x = sequence.pad_sequences(indexes, \n",
    "                               maxlen=sentence_size, \n",
    "                               truncating='post',\n",
    "                               padding='post',\n",
    "                               value=pad_id)\n",
    "    length = np.array([min(len(x), sentence_size) for x in indexes])\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": x, \"len\": length}, shuffle=False)\n",
    "    predictions = {}\n",
    "    for path, classifier in all_classifiers.items():\n",
    "        predictions[path] = [p['logistic'][0] for p in classifier.predict(input_fn=predict_input_fn)]\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        print(sentence)\n",
    "        for path in all_classifiers:\n",
    "            print(\"\\t{} {}\".format(path, predictions[path][idx]))\n",
    "            \n",
    "print_predictions([\n",
    "    'I really liked the movie!',\n",
    "    'Hated every second of it...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuycPjrrvq-d"
   },
   "source": [
    "### Список для чтения\n",
    "\n",
    "Интуиция по работе CNN:\n",
    "* Colah's [Understanding convolutions](http://colah.github.io/posts/2014-07-Understanding-Convolutions/)\n",
    "* Colah's [Conv Nets: A Modular Perspective](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/)\n",
    "* [CS231n](http://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "Туториалы:\n",
    "* Как использовать TensorFlow Estimator API в различных задачах [tf-estimator-tutorials](https://github.com/GoogleCloudPlatform/tf-estimator-tutorials)\n",
    "\n",
    "Со звездочкой:\n",
    "* [Back Propagation in Convolutional Neural Networks — Intuition and Code](https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199)\n",
    "* [Dilated Convolutions and Kronecker Factored Convolutions](https://www.inference.vc/dilated-convolutions-and-kronecker-factorisation/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "nlp_estimators.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
